{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to present a summary of condor commands in a more useful fashion than currently available (e.g. at https://research.cs.wisc.edu/htcondor/manual/current/11_Command_Reference.html)\n",
    "\n",
    "It's also an opportunity to exercise some scraping techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import os\n",
    "import urlparse\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_page_offline(url, filename):\n",
    "    \"\"\"Small function to save webpage offline.\n",
    "    \n",
    "    url: str\n",
    "        URL of the page to save.\n",
    "    filename: str\n",
    "        File to save it in.\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    with open(filename, 'w') as offline:\n",
    "        offline.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docpage = 'https://research.cs.wisc.edu/htcondor/manual/current/11_Command_Reference.html'\n",
    "base_url = os.path.dirname(docpage)  # replace with something better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the online way\n",
    "# r = requests.get(docpage)\n",
    "# soup_main = bs4.BeautifulSoup(r.text, 'lxml')\n",
    "# save_page_offline(docpage, 'doc_offline.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open it from offline file\n",
    "with open('doc_offline.html') as html:\n",
    "    html_text = html.readlines()\n",
    "\n",
    "soup_main = bs4.BeautifulSoup(''.join(html_text), 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a dict instead?\n",
    "class CondorCmd(object):\n",
    "    \"\"\"Holds info about a Condor command\"\"\"\n",
    "    \n",
    "    def __init__(self, name, brief=None, synopsis=None, \n",
    "                 description=None, options=None, url=None):\n",
    "        \"\"\"\n",
    "        name: str\n",
    "            command name\n",
    "        brief: str\n",
    "            Brief summary of what command does.\n",
    "        synopsis: str\n",
    "            Brief summary of command usage.\n",
    "        description: str\n",
    "            Full info about the command.\n",
    "        options: str\n",
    "            Flags and values that the command can use.\n",
    "        url: str\n",
    "            URL with info about the command. \n",
    "            Should be the full page URL. \n",
    "            e.g. 'https://research.cs.wisc.edu/htcondor/manual/current/condor_q.html'\n",
    "            instead of 'condor_q.html'\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.brief = brief\n",
    "        self.synopsis = synopsis\n",
    "        self.description = description\n",
    "        self.options = options\n",
    "        self.url = url       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store all of our CondorCmd objects\n",
    "commands = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the big list, crawl through to get all the possible commands\n",
    "main_list = soup_main.ul\n",
    "for item in main_list.children:\n",
    "    if not isinstance(item, bs4.element.Tag): \n",
    "        continue\n",
    "    cmd_tag = item.find_all('a')[0]\n",
    "    name = cmd_tag.string\n",
    "    page = cmd_tag.attrs['href']\n",
    "    c = CondorCmd(name, url=os.path.join(base_url, page))\n",
    "    commands.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bosco_cluster https://research.cs.wisc.edu/htcondor/manual/current/bosco_cluster.html\n",
      "bosco_findplatform https://research.cs.wisc.edu/htcondor/manual/current/bosco_findplatform.html\n",
      "bosco_install https://research.cs.wisc.edu/htcondor/manual/current/bosco_install.html\n",
      "bosco_ssh_start https://research.cs.wisc.edu/htcondor/manual/current/bosco_ssh_start.html\n",
      "bosco_start https://research.cs.wisc.edu/htcondor/manual/current/bosco_start.html\n",
      "bosco_stop https://research.cs.wisc.edu/htcondor/manual/current/bosco_stop.html\n",
      "bosco_uninstall https://research.cs.wisc.edu/htcondor/manual/current/bosco_uninstall.html\n",
      "condor_advertise https://research.cs.wisc.edu/htcondor/manual/current/condor_advertise.html\n",
      "condor_check_userlogs https://research.cs.wisc.edu/htcondor/manual/current/condor_check_userlogs.html\n",
      "condor_checkpoint https://research.cs.wisc.edu/htcondor/manual/current/condor_checkpoint.html\n",
      "condor_chirp https://research.cs.wisc.edu/htcondor/manual/current/condor_chirp.html\n",
      "condor_cod https://research.cs.wisc.edu/htcondor/manual/current/condor_cod.html\n",
      "condor_compile https://research.cs.wisc.edu/htcondor/manual/current/condor_compile.html\n",
      "condor_config_val https://research.cs.wisc.edu/htcondor/manual/current/condor_config_val.html\n",
      "condor_configure https://research.cs.wisc.edu/htcondor/manual/current/condor_configure.html\n",
      "condor_continue https://research.cs.wisc.edu/htcondor/manual/current/condor_continue.html\n",
      "condor_dagman https://research.cs.wisc.edu/htcondor/manual/current/condor_dagman.html\n",
      "condor_dagman_metrics_reporter https://research.cs.wisc.edu/htcondor/manual/current/condor_dagman_metrics_repor.html\n",
      "condor_drain https://research.cs.wisc.edu/htcondor/manual/current/condor_drain.html\n",
      "condor_fetchlog https://research.cs.wisc.edu/htcondor/manual/current/condor_fetchlog.html\n",
      "condor_findhost https://research.cs.wisc.edu/htcondor/manual/current/condor_findhost.html\n",
      "condor_gather_info https://research.cs.wisc.edu/htcondor/manual/current/condor_gather_info.html\n",
      "condor_gpu_discovery https://research.cs.wisc.edu/htcondor/manual/current/condor_gpu_discovery.html\n",
      "condor_history https://research.cs.wisc.edu/htcondor/manual/current/condor_history.html\n",
      "condor_hold https://research.cs.wisc.edu/htcondor/manual/current/condor_hold.html\n",
      "condor_install https://research.cs.wisc.edu/htcondor/manual/current/condor_install.html\n",
      "condor_job_router_info https://research.cs.wisc.edu/htcondor/manual/current/condor_job_router_info.html\n",
      "condor_master https://research.cs.wisc.edu/htcondor/manual/current/condor_master.html\n",
      "condor_off https://research.cs.wisc.edu/htcondor/manual/current/condor_off.html\n",
      "condor_on https://research.cs.wisc.edu/htcondor/manual/current/condor_on.html\n",
      "condor_ping https://research.cs.wisc.edu/htcondor/manual/current/condor_ping.html\n",
      "condor_pool_job_report https://research.cs.wisc.edu/htcondor/manual/current/condor_pool_job_report.html\n",
      "condor_power https://research.cs.wisc.edu/htcondor/manual/current/condor_power.html\n",
      "condor_preen https://research.cs.wisc.edu/htcondor/manual/current/condor_preen.html\n",
      "condor_prio https://research.cs.wisc.edu/htcondor/manual/current/condor_prio.html\n",
      "condor_procd https://research.cs.wisc.edu/htcondor/manual/current/condor_procd.html\n",
      "condor_q https://research.cs.wisc.edu/htcondor/manual/current/condor_q.html\n",
      "condor_qedit https://research.cs.wisc.edu/htcondor/manual/current/condor_qedit.html\n",
      "condor_qsub https://research.cs.wisc.edu/htcondor/manual/current/condor_qsub.html\n",
      "condor_reconfig https://research.cs.wisc.edu/htcondor/manual/current/condor_reconfig.html\n",
      "condor_release https://research.cs.wisc.edu/htcondor/manual/current/condor_release.html\n",
      "condor_reschedule https://research.cs.wisc.edu/htcondor/manual/current/condor_reschedule.html\n",
      "condor_restart https://research.cs.wisc.edu/htcondor/manual/current/condor_restart.html\n",
      "condor_rm https://research.cs.wisc.edu/htcondor/manual/current/condor_rm.html\n",
      "condor_rmdir https://research.cs.wisc.edu/htcondor/manual/current/condor_rmdir.html\n",
      "condor_router_history https://research.cs.wisc.edu/htcondor/manual/current/condor_router_history.html\n",
      "condor_router_q https://research.cs.wisc.edu/htcondor/manual/current/condor_router_q.html\n",
      "condor_router_rm https://research.cs.wisc.edu/htcondor/manual/current/condor_router_rm.html\n",
      "condor_run https://research.cs.wisc.edu/htcondor/manual/current/condor_run.html\n",
      "condor_set_shutdown https://research.cs.wisc.edu/htcondor/manual/current/condor_set_shutdown.html\n",
      "condor_ssh_to_job https://research.cs.wisc.edu/htcondor/manual/current/condor_ssh_to_job.html\n",
      "condor_sos https://research.cs.wisc.edu/htcondor/manual/current/condor_sos.html\n",
      "condor_stats https://research.cs.wisc.edu/htcondor/manual/current/condor_stats.html\n",
      "condor_status https://research.cs.wisc.edu/htcondor/manual/current/condor_status.html\n",
      "condor_store_cred https://research.cs.wisc.edu/htcondor/manual/current/condor_store_cred.html\n",
      "condor_submit https://research.cs.wisc.edu/htcondor/manual/current/condor_submit.html\n",
      "condor_submit_dag https://research.cs.wisc.edu/htcondor/manual/current/condor_submit_dag.html\n",
      "condor_suspend https://research.cs.wisc.edu/htcondor/manual/current/condor_suspend.html\n",
      "condor_tail https://research.cs.wisc.edu/htcondor/manual/current/condor_tail.html\n",
      "condor_transfer_data https://research.cs.wisc.edu/htcondor/manual/current/condor_transfer_data.html\n",
      "condor_update_machine_ad https://research.cs.wisc.edu/htcondor/manual/current/condor_update_machine_ad.html\n",
      "condor_updates_stats https://research.cs.wisc.edu/htcondor/manual/current/condor_updates_stats.html\n",
      "condor_urlfetch https://research.cs.wisc.edu/htcondor/manual/current/condor_urlfetch.html\n",
      "condor_userlog https://research.cs.wisc.edu/htcondor/manual/current/condor_userlog.html\n",
      "condor_userprio https://research.cs.wisc.edu/htcondor/manual/current/condor_userprio.html\n",
      "condor_vacate https://research.cs.wisc.edu/htcondor/manual/current/condor_vacate.html\n",
      "condor_vacate_job https://research.cs.wisc.edu/htcondor/manual/current/condor_vacate_job.html\n",
      "condor_version https://research.cs.wisc.edu/htcondor/manual/current/condor_version.html\n",
      "condor_wait https://research.cs.wisc.edu/htcondor/manual/current/condor_wait.html\n",
      "condor_who https://research.cs.wisc.edu/htcondor/manual/current/condor_who.html\n",
      "gidd_alloc https://research.cs.wisc.edu/htcondor/manual/current/gidd_alloc.html\n",
      "procd_ctl https://research.cs.wisc.edu/htcondor/manual/current/procd_ctl.html\n"
     ]
    }
   ],
   "source": [
    "for c in commands: \n",
    "    print c.name, c.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grab_cmd_info(url, cmd):\n",
    "    \"\"\"Grab from the URL information about the command \n",
    "    including synopsis, description, options, etc.\n",
    "\n",
    "    Returns a dict of information.\n",
    "\n",
    "    url: str\n",
    "        Full URL from which to get command information.\n",
    "    cmd: str\n",
    "        Condor command\n",
    "    \"\"\"\n",
    "#     rc = requests.get(url)\n",
    "#     soup_cmd = bs4.BeautifulSoup(rc.text, 'lxml')\n",
    "#     save_page_offline(url, os.path.basename(url))\n",
    "\n",
    "    # get it from offline\n",
    "    with open(os.path.basename(url)) as rc_offline:\n",
    "        cmd_html_text = rc_offline.readlines()\n",
    "    soup_cmd = bs4.BeautifulSoup(''.join(cmd_html_text), 'lxml')\n",
    "\n",
    "    # get body text, sanitise\n",
    "    body = soup_cmd.body.text \n",
    "    body = re.sub(r'\\n\\n+', '\\n', body) # get rid of extra lines\n",
    "    body = re.sub(r'\\n[\\s\\xa0]+', '\\n', body) # remove leading spaces\n",
    "    body = re.sub(r'[\\s\\xa0]+\\n', '\\n', body) # remove trailing spaces\n",
    "\n",
    "    info = {}\n",
    "\n",
    "    # get brief description\n",
    "    p_brief = re.compile(cmd+r'\\n([\\d\\w.()-/ \\'\\n]*)\\nSynopsis', re.IGNORECASE)\n",
    "    brief_search = p_brief.search(body)\n",
    "    if brief_search:\n",
    "        info['brief'] = brief_search.group(1).replace('\\n', ' ')\n",
    "    else:\n",
    "        print 'No brief info for', cmd\n",
    "        info['brief'] = None\n",
    "        \n",
    "    # get synopsis\n",
    "    p_synopsis = re.compile(r'Synopsis\\n('+cmd+r'.+)\\nDescription', re.IGNORECASE | re.DOTALL)\n",
    "    synopsis_search = p_synopsis.search(body)\n",
    "    if synopsis_search:\n",
    "        synopsis_raw = synopsis_search.group(1)\n",
    "        info['synopsis'] = [i.replace('\\n', ' ').strip() for i in synopsis_raw.split(cmd) if i]\n",
    "    else:\n",
    "        print 'No synopsis info for', cmd\n",
    "        info['synopsis'] = None\n",
    "        \n",
    "    # get description\n",
    "#     p_desc = re.compile(r'Description\\n(.*?)\\nOptions', re.IGNORECASE | re.DOTALL)\n",
    "#     print p_desc.findall(body)\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No brief info for bosco_findplatform\n",
      "No brief info for bosco_install\n",
      "No brief info for bosco_ssh_start\n",
      "No synopsis info for bosco_ssh_start\n",
      "No synopsis info for bosco_start\n",
      "No synopsis info for bosco_stop\n",
      "No synopsis info for bosco_uninstall\n",
      "No synopsis info for condor_install\n",
      "No synopsis info for condor_master\n",
      "No synopsis info for condor_pool_job_report\n"
     ]
    }
   ],
   "source": [
    "for c in commands:\n",
    "#     print c.name, c.url\n",
    "    info_dict = grab_cmd_info(c.url, c.name)\n",
    "    c.brief = info_dict['brief']\n",
    "    c.synopsis = info_dict['synopsis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bosco_cluster Manage and configure the clusters to be accessed. [u'[-h || --help]', u'[-l || --list] [-a || --add host schedd] [-r || --remove host] [-s || --status host] [-t || --test host]']\n",
      "bosco_findplatform None [u'[-h || --help]', u'[-u || --url] [-b || --bit] [-f || --full] [--force=<platformstring>] [-i || --install <installoptions>]']\n",
      "bosco_install None [u'[--help] | [--usage]', u'[--install[=<path/to/release_dir>]] [--prefix=<path>] [--install-dir=<path>] [--local-dir=<path>] [--make-personal-condor] [--bosco] [--type=<[submit][,execute][,manager]>] [--central-manager=<host>] [--credd] [--owner=<username>] [--maybe-daemon-owner] [--install-log=<file>] [--overwrite] [--env-scripts-dir=<dir>] [--no-env-scripts] [--ignore-missing-libs] [--force] [--backup] [--verbose]']\n",
      "bosco_ssh_start None None\n",
      "bosco_start start up the Personal HTCondor installation specific to Bosco None\n",
      "bosco_stop Shut down HTCondor daemons in a Bosco installation. None\n",
      "bosco_uninstall uninstall a Bosco installation None\n",
      "condor_advertise Send a ClassAd to the condor_collector daemon [u'[-help  -version]', u'[-pool  centralmanagerhostname[:portname]] [-debug] [-tcp] [-multiple] update-command [classad-filename]']\n",
      "condor_check_userlogs Check job event log files for errors [u'UserLogFile1 [UserLogFile2 ... UserLogFileN ]']\n",
      "condor_checkpoint send a checkpoint command to jobs running on specified hosts [u'[-help  -version]', u'[-debug] [-pool  centralmanagerhostname[:portnumber]] [-name hostname  hostname -addr \"a.b.c.d:port\"  \"a.b.c.d:port\" -constraint expression  -all ]']\n",
      "condor_chirp Access files or job ClassAd from an executing job [u'<Chirp-Command> Description', u'is not intended for use as a command-line tool. It is most often invoked by an HTCondor job, while the job is executing. It accesses files or job ClassAd attributes on the submit machine. Files can be read, written or removed. Job attributes can be read, and most attributes can be updated. When invoked by an HTCondor job, the command-line arguments describe the operation to be performed. Each of these arguments is described below within the section on Chirp Commands.']\n",
      "condor_cod manage COD machines and jobs [u'[-help  -version]', u'request [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] [[-help  -version]  [-debug  -timeout N  -classad file] ] [-requirements expr] [-lease N]', u'release -id ClaimID [[-help  -version]  [-debug  -timeout N  -classad file] ] [-fast]', u'activate -id ClaimID [[-help  -version]  [-debug  -timeout N  -classad file] ] [-keyword string  -jobad filename  -cluster N  -proc N  -requirements expr]', u'deactivate -id ClaimID [[-help  -version]  [-debug  -timeout N  -classad file] ] [-fast]', u'suspend -id ClaimID [[-help  -version]  [-debug  -timeout N  -classad file] ]', u'renew -id ClaimID [[-help  -version]  [-debug  -timeout N  -classad file] ]', u'resume -id ClaimID [[-help  -version]  [-debug  -timeout N  -classad file] ]', u'delegate_proxy -id ClaimID [[-help  -version]  [-debug  -timeout N  -classad file] ] [-x509proxy  ProxyFile]']\n",
      "condor_compile create a relinked executable for use as a standard universe job [u'cc |  CC |  gcc |  f77 |  g++ |  ld |  make |  ...']\n",
      "condor_config_val Query or set a given HTCondor configuration variable [u'<help option>', u'[<location options>] <edit option>', u'[<location options>] [<view options>] vars']\n",
      "condor_configure Configure or install HTCondor [u'or condor_install [--help] [--usage]', u'or condor_install [--install[=path/to/release] ] [--install-dir=path] [--prefix=path] [--local-dir=path] [--make-personal-condor] [--bosco] [--type =  submit, execute, manager ] [--central-manager =  hostname] [--owner =  ownername ] [--maybe-daemon-owner] [--install-log =  file ] [--overwrite] [--ignore-missing-libs] [--force] [--no-env-scripts] [--env-scripts-dir =  directory ] [--backup] [--credd] [--verbose]']\n",
      "condor_continue continue suspended jobs from the HTCondor queue [u'[-help  -version]', u'[-debug] [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] cluster  cluster.process  user -constraint expression  -all']\n",
      "condor_dagman meta scheduler of the jobs submitted as the nodes of a DAG or DAGs [u'-f -t -l . -help', u'-version', u'-f -l . -csdversion version_string [-debug  level] [-maxidle  numberOfJobs] [-maxjobs  numberOfJobs] [-maxpre  NumberOfPREscripts] [-maxpost  NumberOfPOSTscripts] [-noeventchecks] [-allowlogerror] [-usedagdir] -lockfile filename [-waitfordebug] [-autorescue  0|1] [-dorescuefrom  number] [-allowversionmismatch] [-DumpRescue] [-verbose] [-force] [-notification  value] [-suppress_notification] [-dont_suppress_notification] [-dagman  DagmanExecutable] [-outfile_dir  directory] [-update_submit] [-import_env] [-priority  number] [-dont_use_default_node_log] [-DontAlwaysRunPost] [-DoRecovery] -dag dag_file [-dag dag_file_2 ... -dag dag_file_n ]']\n",
      "condor_dagman_metrics_reporter Report the statistics of a DAGMan run to a central HTTP server [u'[-s] [-u  URL] [-t  maxtime] -f /path/to/metrics/file']\n",
      "condor_drain Control draining of an execute machine [u'[-help]', u'[-debug] [-pool  pool-name] [-graceful | -quick | -fast] [-resume-on-completion] [-check  expr] machine-name', u'[-debug] [-pool  pool-name] -cancel [-request-id  id] machine-name']\n",
      "condor_fetchlog Retrieve a daemon's log file that is located on another computer [u'[-help  -version]', u'[-pool  centralmanagerhostname[:portnumber]] [-master  -startd  -schedd  -collector  -negotiator -kbdd] machine-name subsystem[.extension]']\n",
      "condor_findhost find machine(s) in the pool that can be used with minimal impact on currently running HTCondor jobs and best meet any specified constraints [u'[-help] [-m] [-n  num] [-c  c_expr] [-r  r_expr] [-p  centralmanagerhostname]']\n",
      "condor_gather_info Gather information about an HTCondor installation and a queued job [u'[--jobid ClusterId.ProcId] [--scratch /path/to/directory]']\n",
      "condor_gpu_discovery Output GPU-related ClassAd attributes [u'-help', u'[<options>]']\n",
      "condor_history View log of HTCondor jobs completed to date [u'[-help]', u'[-name  name] [-pool  centralmanagerhostname[:portnumber]] [-backwards] [-forwards] [-constraint  expr] [-file  filename] [-userlog  filename] [-format  formatString AttributeName] [-autoformat[:tn,lVh]  attr1 [attr2 ...]] [-l | -long | -xml] [-match | -limit  number] [cluster | cluster.process | owner]']\n",
      "condor_hold put jobs in the queue into the hold state [u'[-help  -version]', u'[-debug] [-reason  reasonstring] [-subcode  number] [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] cluster...  cluster.process...  user... -constraint expression ...', u'[-debug] [-reason  reasonstring] [-subcode  number] [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] -all']\n",
      "condor_install Configure or install HTCondor None\n",
      "condor_job_router_info Discover and display information related to job routing [u'[-help  -version]', u'-config', u'-match-jobs -jobads filename [-ignore-prior-routing]']\n",
      "condor_master The master HTCondor Daemon None\n",
      "condor_off Shutdown HTCondor daemons [u'[-help  -version]', u'[-graceful  -fast   -peaceful  -force-graceful] [-debug] [-pool  centralmanagerhostname[:portnumber]] [-name hostname  hostname -addr \"a.b.c.d:port\"  \"a.b.c.d:port\" -constraint expression  -all ] [-daemon  daemonname]']\n",
      "condor_on Start up HTCondor daemons [u'[-help  -version]', u'[-debug] [-pool  centralmanagerhostname[:portnumber]] [-name hostname  hostname -addr \"a.b.c.d:port\"  \"a.b.c.d:port\" -constraint expression  -all ] [-daemon  daemonname]']\n",
      "condor_ping Attempt a security negotiation to determine if it succeeds [u'[-help  -version]', u'[-debug] [-address  <a.b.c.d:port>] [-pool  host name] [-name  daemon name] [-type  subsystem] [-config  filename] [-quiet | -table | -verbose] token [token [... ]]']\n",
      "condor_pool_job_report generate report about all jobs that have run in the last 24 hours on all execute hosts None\n",
      "condor_power send packet intended to wake a machine from a low power state [u'[-h]', u'[-d] [-i] [-m  MACaddress] [-s  subnet] [ClassAdFile]']\n",
      "condor_preen remove extraneous files from HTCondor directories [u'[-mail] [-remove] [-verbose] [-debug]']\n",
      "condor_prio change priority of jobs in the HTCondor queue [u'-p priority  + value  - value [-n  schedd_name] cluster | cluster.process | username | -a', u'-p priority  + value  - value [-pool pool_name -n schedd_name ] cluster | cluster.process | username | -a']\n",
      "condor_procd Track and manage process families [u'-h', u'-A address-file [options]']\n",
      "condor_q Display information about jobs in queue [u'[-help [Universe | State]]', u'[-debug] [general options] [restriction list] [output options] [analyze options]']\n",
      "condor_qedit modify job attributes [u'[-debug] [-n  schedd-name] [-pool  pool-name] {cluster | cluster.proc | owner | -constraint constraint} attribute-name attribute-value ...']\n",
      "condor_qsub Queue jobs that use PBS/SGE-style submission [u'[--version]', u'[Specific options] [Directory options] [Environmental options] [File options] [Notification options] [Resource options] [Status options] [Submission options] commandfile Description', u'submits an HTCondor job. This job is specified in a PBS/Torque style or an SGE style.', u\"permits the submission of dependent jobs without the need to specify the full dependency graph at submission time. Doing things this way is neither as efficient as HTCondor's DAGMan, nor as functional as SGE's qsub or qalter.\", u'serves as a minimal translator to be able to use software originally written to interact with PBS, Torque, and SGE in an HTCondor pool.', u'attempts to behave like qsub. Less than half of the qsub functionality is implemented. Option descriptions describe the differences between the behavior of qsub and', u'. qsub options not listed here are not supported. Some concepts present in PBS and SGE do not apply to HTCondor, and so these options are not implemented. For a full listing of qsub options, please see POSIX : http://pubs.opengroup.org/onlinepubs/9699919799/utilities/qsub.html SGE : http://gridscheduler.sourceforge.net/htmlman/htmlman1/qsub.html PBS/Torque : http://docs.adaptivecomputing.com/torque/4-1-3/Content/topics/commands/qsub.htm', u'accepts either command line options or the single file, commandfile, that contains all of the commands.', u'does the opposite of job submission within the grid universe batch grid type, which takes HTCondor jobs submitted with HTCondor syntax and submits them to PBS, SGE, or LSF. Options -a date_time (Submission option) Specify a deferred execution date and time. The PBS/Torque syntax of date_time is a string in the form [[[[CC]YY]MM]DD]hhmm[.SS]. The portions of this string which are optional are CC, YY, MM, DD, and SS. For SGE, MM and DD are not optional. For PBS, MM and DD are optional.', u'follows the PBS style. -A account_string (Status option) Uses group accounting where the string account_string is the accounting group associated with this job. Unlike SGE, there is no default group of \"sge\". -b y|n (Submission option) Using the SGE definition of its -b option, a value of y causes', u'to not parse the file for additional', u'commands. The default value is n. If the command line argument -f filename is also specified, it negates a value of y. -c checkpoint_option (Submission option) For standard universe jobs only, controls the how HTCondor produces checkpoints.  checkpoint_options may be one of n or N Do not produce checkpoints. s or S Do not produce periodic checkpoints.  A job will only produce a checkpoint when the job is evicted. More options may be implemented in the future. --condor-keep-files (Specific option) Directs HTCondor to not remove temporary files generated by', u', such as HTCondor submit files and sentinel jobs. These temporary files may be important for debugging. -cwd (Directory option) Specifies the initial directory in which the job will run to be the current directory from which the job was submitted. This sets initialdir for condor_submit. -d path or -wd path (Directory option) Specifies the initial directory in which the job will run to be path. This sets initialdir for condor_submit. -e filename (File option) Specifies the condor_submit command error, the file where stderr is written. If not specified, set to the default name of <commandfile>.e<ClusterId>, where <commandfile> is the', u'argument, and  <ClusterId> is the job attribute ClusterId assigned for the job. --f qsub_file (Specific option) Parse qsub_file to search for and set additional condor_submit commands. Within the file, commands will appear as #PBS or #SGE.', u\"will parse the batch file listed as qsub_file. -h (Status option) Placed submitted job directly into the hold state. --help (Specific option) Print usage information and exit. -hold_jid <jid> (Status option) Submits a job in the hold state.  This job is released only when a previously submitted job, identified by its cluster ID as <jid>, exits successfully. Successful completion is defined as not exiting with exit code 100. In implementation, there are three jobs that define this SGE feature. The first job is the previously submitted job. The second job is the newly submitted one that is waiting for the first to finish successfully. The third job is what SGE calls a sentinel job; this is an HTCondor local universe job that watches the history for the first job's exit code.  This third job will exit once it has seen the exit code and, for a successful termination of the first job, run condor_release on the second job. If the first job is an array job, the second job will only be released after all individual jobs of the first job have completed. -i  [hostname:]filename (File option) Specifies the condor_submit command input, the file from which stdin is read. -j characters (File option) Acceptable characters for this option are e, o, and n. The only sequence that is relevant is eo; it specifies that both standard output and standard error are to be sent to the same file. The file will be the one specified by the -o option, if both the -o and -e options exist. The file will be the one specified by the -e option, if only the -e option is provided. If neither the -o nor the -e options are provided, the file will be the default used for the -o option. -l resource_spec (Resource option) Specifies requirements for the job, such as the amount of RAM and the number of CPUs. Only PBS-style resource requests are supported. resource_spec is a comma separated list of key/value pairs. Each pair is of the form resource_name=value. resource_name and value may be resource_name value\"]\n",
      "condor_reconfig Reconfigure HTCondor daemons [u'[-help  -version]', u'[-debug] [-pool  centralmanagerhostname[:portnumber]] [-name hostname  hostname -addr \"a.b.c.d:port\"  \"a.b.c.d:port\" -constraint expression  -all ] [-daemon  daemonname]']\n",
      "condor_release release held jobs in the HTCondor queue [u'[-help  -version]', u'[-debug] [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] cluster...  cluster.process...  user... -constraint expression ...', u'[-debug] [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] -all']\n",
      "condor_reschedule Update scheduling information to the central manager [u'[-help  -version]', u'[-debug] [-pool  centralmanagerhostname[:portnumber]] [-name hostname  hostname -addr \"a.b.c.d:port\"  \"a.b.c.d:port\" -constraint expression  -all ]']\n",
      "condor_restart Restart a set of HTCondor daemons [u'[-help  -version]', u'[-debug] [-graceful  -fast   -peaceful] [-pool  centralmanagerhostname[:portnumber]] [-name hostname  hostname -addr \"a.b.c.d:port\"  \"a.b.c.d:port\" -constraint expression  -all ] [-daemon  daemonname]']\n",
      "condor_rm remove jobs from the HTCondor queue [u'[-help  -version]', u'[-debug] [-forcex] [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] cluster...  cluster.process...  user... -constraint expression ...', u'[-debug] [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] -all']\n",
      "condor_rmdir Windows-only no-fail deletion of directories [u'[/HELP | /?]', u'@filename', u'[/VERBOSE] [/DIAGNOSTIC] [/PATH:<path>] [/S] [/C] [/Q] [/NODEL] directory']\n",
      "condor_router_history Display the history for routed jobs [u'[--h]', u'[--show_records] [--show_iwd] [--age days] [--days days] [--start \"YYYY-MM-DD HH:MM\"]']\n",
      "condor_router_q Display information about routed jobs in the queue [u'[-S] [-R] [-I] [-H] [-route  name] [-idle] [-held] [-constraint  X] [condor_q options]']\n",
      "condor_router_rm Remove jobs being managed by the HTCondor Job Router [u'[router_rm options] [condor_rm options]']\n",
      "condor_run Submit a shell command-line as an HTCondor job [u'[-u  universe] [-a  submitcmd] \"shell command\"']\n",
      "condor_set_shutdown Set a program to execute upon condor_master shut down [u'[-help  -version]', u'-exec programname [-debug] [-pool  centralmanagerhostname[:portnumber]] [-name hostname  hostname -addr \"a.b.c.d:port\"  \"a.b.c.d:port\" -constraint expression  -all ]']\n",
      "condor_ssh_to_job create an ssh session to a running job [u'[-help]', u'[-debug] [-name  schedd-name] [-pool  pool-name] [-ssh  ssh-command] [-keygen-options  ssh-keygen-options] [-shells  shell1,shell2,...] [-auto-retry] [-remove-on-interrupt] cluster | cluster.process | cluster.process.node [remote-command]']\n",
      "condor_sos Issue a command that will be service with a higher priority [u'[-help  -version]', u'[-debug] [-timeoutmult  value] condor_command']\n",
      "condor_stats Display historical information about the HTCondor pool [u'[-f  filename] [-orgformat] [-pool  centralmanagerhostname[:portnumber]] [time-range] query-type']\n",
      "condor_status Display status of the HTCondor pool [u'[-debug] [help options] [query options] [display options] [custom options] [name ... ]']\n",
      "condor_store_cred securely stash a password [u'[-help]', u'add [ -c | -u username ] [-p  password] [-n  machinename] [-f  filename]', u'delete [ -c | -u username ] [-n  machinename]', u'query [ -c | -u username ] [-n  machinename]']\n",
      "condor_submit Queue jobs for execution under HTCondor [u'[-verbose] [-unused] [-name  schedd_name] [-remote  schedd_name] [-addr  ip:port] [-pool  pool_name] [-disable] [-password  passphrase] [-debug] [-append command ... ] [-spool] [-dump  filename] [-interactive] [-dry-run] [-maxjobs  <number-of-jobs>] [-single-cluster] [<submit-variable>=<value>] [submit description file] [-queue  queue_arguments] Description', u'is the program for submitting jobs for execution under HTCondor.', u'requires a submit description file which contains commands to direct the queuing of jobs. One submit description file may contain specifications for the queuing of many HTCondor jobs at once. A single invocation of', u\"may cause one or more clusters. A cluster is a set of jobs specified in the submit description file between queue commands for which the executable is not changed. It is advantageous to submit multiple jobs as a single cluster because: Only one copy of the checkpoint file is needed to represent all jobs in a cluster until they begin execution. There is much less overhead involved for HTCondor to start the next job in a cluster than for HTCondor to start a new cluster.  This can make a big difference when submitting lots of short jobs. Multiple clusters may be specified within a single submit description file. Each cluster must specify a single executable. The job ClassAd attribute ClusterId identifies a cluster. The submit description file argument is the path and file name of the submit description file. If this optional argument is missing or is the dash character (-), then the commands are taken from standard input. Note that submission of jobs from a Windows machine requires a stashed password to allow HTCondor to impersonate the user submitting the job. To stash a password, use the condor_store_cred command. See the manual page for details. For lengthy lines within the submit description file, the backslash ( ) is a line continuation character. Placing the backslash at the end of a line causes the current line's command to be continued with the next line of the file. Submit description files may contain comments. A comment is any line beginning with a pound character (#). Options -verbose Verbose output - display the created job ClassAd -unused As a default, causes no warnings to be issued about user-defined macros not being used within the submit description file. The meaning reverses (toggles) when the configuration variable WARN_ON_UNUSED_SUBMIT_FILE_MACROS is set to the non default value of False. Printing the warnings can help identify spelling errors of submit description file commands.  The warnings are sent to stderr. -name schedd_name Submit to the specified condor_schedd. Use this option to submit to a condor_schedd other than the default local one. schedd_name is the value of the Name ClassAd attribute on the machine where the condor_schedd daemon runs. -remote schedd_name Submit to the specified condor_schedd, spooling all required input files over the network connection. schedd_name is the value of the Name ClassAd attribute on the machine where the condor_schedd daemon runs. This option is equivalent to using both -name and -spool. -addr ip:port Submit to the condor_schedd at the IP address and port given by the sinful string argument ip:port. -pool pool_name Look in the specified pool for the condor_schedd to submit to. This option is used with -name or -remote. -disable Disable file permission checks when submitting a job for read permissions on all input files, such as those defined by commands input and transfer_input_files, as well as write permission to output files, such as a log file defined by log and output files defined with output or transfer_output_files. -password passphrase Specify a password to the MyProxy server. -debug Cause debugging information to be sent to stderr, based on the value of the configuration variable TOOL_DEBUG. -append command Augment the commands in the submit description file with the given command. This command will be considered to immediately precede the queue command within the submit description file, and come after all other previous commands. If the command specifies a queue command, as in the example\", u'mysubmitfile -append \"queue input in A, B, C\" then the entire -append command line option and its arguments are converted to', u'mysubmitfile -queue input in A, B, C The submit description file is not modified. Multiple commands are specified by using the -append option multiple times. Each new command is given in a separate -append option. Commands with spaces in them will need to be enclosed in double quote marks. -spool Spool all required input files, job event log, and proxy over the connection to the condor_schedd. After submission, modify local copies of the files without affecting your jobs. Any output files for completed jobs need to be retrieved with condor_transfer_data. -dump filename Sends all ClassAds to the specified file, instead of to the condor_schedd. -interactive Indicates that the user wants to run an interactive shell on an execute machine in the pool. This is equivalent to creating a submit description file of a vanilla universe sleep job, and then running condor_ssh_to_job by hand. Without any additional arguments,', u'with the -interactive flag creates a dummy vanilla universe job that sleeps, submits it to the local scheduler, waits for the job to run, and then launches condor_ssh_to_job to run a shell. If the user would like to run the shell on a machine that matches a particular requirements expression, the submit description file is specified, and it will contain the expression. Note that all policy expressions specified in the submit description file are honored, but any executable or universe commands are overwritten to be sleep and vanilla. The job ClassAd attribute InteractiveJob is set to True to identify interactive jobs for condor_startd policy usage. -dry-run Parse the submit description file, sending the job ClassAd to stderr, but do not submit the job(s). This permits an observation of job specification, and it facilitates debugging the submit description file contents. -maxjobs <number-of-jobs> If the total number of jobs specified by the submit description file is more than the integer value given by <number-of-jobs>, then no jobs are submitted for execution and an error message is generated. A 0 or negative value for the <number-of-jobs> causes no limit to be imposed. -single-cluster If the jobs specified by the submit description file causes more than a single cluster value to be assigned, then no jobs are submitted for execution and an error message is generated. <submit-variable>=<value> Defines a submit command or submit variable with a value, and parses it as if it was placed at the beginning of the submit description file. The submit description file is not changed. To correctly parse the', u'command line, this option must be specified without white space characters before and after the equals sign (=), or the entire option must be surrounded by double quote marks. -queue queue_arguments A command line specification of how many jobs to queue, which is only permitted if the submit description file does not have a queue command. The queue_arguments are the same as may be within a submit description file. The parsing of the queue_arguments finishes at the end of the line or when a dash character (-) is encountered. Therefore, its best placement within the command line will be at the end of the command line. On a Unix command line, the shell expands file globs before parsing occurs. Submit Description File Commands Each submit description file describes one cluster of jobs to be placed in the HTCondor execution pool. All jobs in a cluster must share the same executable, but they may have different input and output files, and different program arguments. The submit description file is the only command-line argument to', u'. If the submit description file argument is omitted,', u'will read the submit description from standard input. The submit description file must contain one executable command and at least one queue command. All of the other commands have default actions. The commands which can appear in the submit description file are numerous.  They are listed here in alphabetical order by category. BASIC COMMANDS arguments = argument_list List of arguments to be supplied to the executable as part of the command line. In the java universe, the first argument must be the name of the class containing main. There are two permissible formats for specifying arguments, identified as the old syntax and the new syntax. The old syntax supports white space characters within arguments only in special circumstances; when used, the command line arguments are represented in the job ClassAd attribute Args. The new syntax supports uniform quoting of white space characters within arguments; when used, the command line arguments are represented in the job ClassAd attribute Arguments. Old Syntax In the old syntax, individual command line arguments are delimited (separated) by space characters. To allow a double quote mark in an argument, it is escaped with a backslash; that is, the two character sequence \\\\\" becomes a single double quote mark within an argument. Further interpretation of the argument string differs depending on the operating system.  On Windows, the entire argument string is passed verbatim (other than the backslash in front of double quote marks) to the Windows application. Most Windows applications will allow spaces within an argument value by surrounding the argument with double quotes marks. In all other cases, there is no further interpretation of the arguments. Example: arguments = one \\\\\"two\\\\\" \\'three\\' Produces in Unix vanilla universe: argument 1: one argument 2: \"two\" argument 3: \\'three\\' New Syntax Here are the rules for using the new syntax: The entire string representing the command line arguments is surrounded by double quote marks. This permits the white space characters of spaces and tabs to potentially be embedded within a single argument. Putting the double quote mark within the arguments is accomplished by escaping it with another double quote mark. The white space characters of spaces or tabs delimit arguments. To embed white space characters of spaces or tabs within a single argument, surround the entire argument with single quote marks. To insert a literal single quote mark, escape it within an argument already delimited by single quote marks by adding another single quote mark. Example: arguments = \"3 simple arguments\" Produces: argument 1: 3 argument 2: simple argument 3: arguments Another example: arguments = \"one \\'two with spaces\\'\\t3\" Produces: argument 1: one argument 2: two with spaces argument 3: 3 And yet another example: arguments = \"one \"\"two\"\" \\'spacey \\'\\'quoted\\'\\' argument\\'\" Produces: argument 1: one argument 2: \"two\" argument 3: spacey \\'quoted\\' argument Notice that in the new syntax, the backslash has no special meaning. This is for the convenience of Windows users. environment = parameter_list List of environment variables. There are two different formats for specifying the environment variables: the old format and the new format.  The old format is retained for backward-compatibility.  It suffers from a platform-dependent syntax and the inability to insert some special characters into the environment. The new syntax for specifying environment values: Put double quote marks around the entire argument string.  This distinguishes the new syntax from the old. The old syntax does not have double quote marks around it. Any literal double quote marks within the string must be escaped by repeating the double quote mark. Each environment entry has the form <name>=<value> Use white space (space or tab characters) to separate environment entries. To put any white space in an environment entry, surround the space and as much of the surrounding entry as desired with single quote marks. To insert a literal single quote mark, repeat the single quote mark anywhere inside of a section surrounded by single quote marks. Example: environment = \"one=1 two=\"\"2\"\" three=\\'spacey \\'\\'quoted\\'\\' value\\'\" Produces the following environment entries: one=1 two=\"2\" three=spacey \\'quoted\\' value Under the old syntax, there are no double quote marks surrounding the environment specification.  Each environment entry remains of the form <name>=<value> Under Unix, list multiple environment entries by separating them with a semicolon (;).  Under Windows, separate multiple entries with a vertical bar (| ).  There is no way to insert a literal semicolon under Unix or a literal vertical bar under Windows.  Note that spaces are accepted, but rarely desired, characters within parameter names and values, because they are treated as literal characters, not separators or ignored white space.  Place spaces within the parameter list only if required. A Unix example: environment = one=1;two=2;three=\"quotes have no \\'special\\' meaning\" This produces the following: one=1 two=2 three=\"quotes have no \\'special\\' meaning\" If the environment is set with the environment command and getenv is also set to true, values specified with environment override values in the submitter\\'s environment (regardless of the order of the environment and getenv commands). error = pathname A path and file name used by HTCondor to capture any error messages the program would normally write to the screen (that is, this file becomes stderr). A path is given with respect to the file system of the machine on which the job is submitted. The file is written (by the job) in the remote scratch directory of the machine where the job is executed. When the job exits, the resulting file is transferred back to the machine where the job was submitted, and the path is utilized for file placement. If not specified, the default value of /dev/null is used for submission to a Unix machine. If not specified, error messages are ignored for submission to a Windows machine. More than one job should not use the same error file, since this will cause one job to overwrite the errors of another. The error file and the output file should not be the same file as the outputs will overwrite each other or be lost. For grid universe jobs, error may be a URL that the Globus tool globus_url_copy understands. executable = pathname An optional path and a required file name of the executable file for this job cluster. Only one executable command within a submit description file is guaranteed to work properly. More than one often works. If no path or a relative path is used, then the executable file is presumed to be relative to the current working directory of the user as the', u'command is issued. If submitting into the standard universe, then the named executable must have been re-linked with the HTCondor libraries (such as via the condor_compile command). If submitting into the vanilla universe (the default), then the named executable need not be re-linked and can be any process which can run in the background (shell scripts work fine as well).  If submitting into the Java universe, then the argument must be a compiled .class file. getenv = True |  False If getenv is set to True, then', u\"will copy all of the user's current shell environment variables at the time of job submission into the job ClassAd. The job will therefore execute with the same set of environment variables that the user had at submit time. Defaults to False. If the environment is set with the environment command and getenv is also set to true, values specified with environment override values in the submitter's environment (regardless of the order of the environment and getenv commands). input = pathname HTCondor assumes that its jobs are long-running, and that the user will not wait at the terminal for their completion. Because of this, the standard files which normally access the terminal, (stdin, stdout, and stderr), must refer to files. Thus, the file name specified with input should contain any keyboard input the program requires (that is, this file becomes stdin). A path is given with respect to the file system of the machine on which the job is submitted. The file is transferred before execution to the remote scratch directory of the machine where the job is executed. If not specified, the default value of /dev/null is used for submission to a Unix machine. If not specified, input is ignored for submission to a Windows machine. For grid universe jobs, input may be a URL that the Globus tool globus_url_copy understands. Note that this command does not refer to the command-line arguments of the program.  The command-line arguments are specified by the arguments command. log = pathname Use log to specify a file name where HTCondor will write a log file of what is happening with this job cluster, called a job event log. For example, HTCondor will place a log entry into this file when and where the job begins running, when the job produces a checkpoint, or moves (migrates) to another machine, and when the job completes. Most users find specifying a log file to be handy; its use is recommended. If no log entry is specified, HTCondor does not create a log for this cluster. If a relative path is specified, it is relative to the current working directory as the job is submitted or the directory specified by submit command initialdir on the submit machine. log_xml = True |  False If log_xml is True, then the job event log file will be written in ClassAd XML. If not specified, XML is not used. Note that the file is an XML fragment; it is missing the file header and footer. Do not mix XML and non-XML within a single file. If multiple jobs write to a single job event log file, ensure that all of the jobs specify this option in the same way. notification = Always |  Complete |  Error |  Never Owners of HTCondor jobs are notified by e-mail when certain events occur. If defined by Always, the owner will be notified whenever the job produces a checkpoint, as well as when the job completes. If defined by Complete, the owner will be notified when the job terminates. If defined by Error, the owner will only be notified if the job terminates abnormally, or if the job is placed on hold because of a failure, and not by user request. If defined by Never (the default), the owner will not receive e-mail, regardless to what happens to the job. The HTCondor User's manual documents statistics included in the e-mail. notify_user = email-address Used to specify the e-mail address to use when HTCondor sends e-mail about a job.  If not specified, HTCondor defaults to using the e-mail address defined by job-owner@UID_DOMAIN where the configuration variable UID_DOMAIN is specified by the HTCondor site administrator. If UID_DOMAIN has not been specified, HTCondor sends the e-mail to: job-owner@submit-machine-name output = pathname The output file captures any information the program would ordinarily write to the screen (that is, this file becomes stdout). A path is given with respect to the file system of the machine on which the job is submitted. The file is written (by the job) in the remote scratch directory of the machine where the job is executed. When the job exits, the resulting file is transferred back to the machine where the job was submitted, and the path is utilized for file placement. If not specified, the default value of /dev/null is used for submission to a Unix machine. If not specified, output is ignored for submission to a Windows machine. Multiple jobs should not use the same output file, since this will cause one job to overwrite the output of another. The output file and the error file should not be the same file as the outputs will overwrite each other or be lost. For grid universe jobs, output may be a URL that the Globus tool globus_url_copy understands. Note that if a program explicitly opens and writes to a file, that file should not be specified as the output file. priority = integer An HTCondor job priority can be any integer, with 0 being the default. Jobs with higher numerical priority will run before jobs with lower numerical priority. Note that this priority is on a per user basis. One user with many jobs may use this command to order his/her own jobs, and this will have no effect on whether or not these jobs will run ahead of another user's jobs. queue [number-of-procs] Places one or more copies of the job into the HTCondor queue. The optional argument number-of-procs specifies how many times to submit the job to the queue, and it defaults to 1. If desired, any commands may be placed between subsequent queue commands, such as new input, output, error, initialdir, or arguments commands. This is handy when submitting multiple runs into one cluster with one submit description file. universe = vanilla |  standard |  scheduler |  local |  grid |  java |  vm |  docker Specifies which HTCondor universe to use when running this job.  The HTCondor universe specifies an HTCondor execution environment. The vanilla universe is the default (except where the configuration variable DEFAULT_UNIVERSE defines it otherwise), and is an execution environment for jobs which do not use HTCondor's mechanisms for taking checkpoints; these are ones that have not been linked with the HTCondor libraries. Use the vanilla universe to submit shell scripts to HTCondor. The standard universe tells HTCondor that this job has been re-linked via condor_compile with the HTCondor libraries and therefore supports taking checkpoints and remote system calls. The scheduler universe is for a job that is to run on the machine where the job is submitted. This universe is intended for a job that acts as a metascheduler and will not be preempted. The local universe is for a job that is to run on the machine where the job is submitted. This universe runs the job immediately and will not be preempt the job. The grid universe forwards the job to an external job management system. Further specification of the grid universe is done with the grid_resource command. The java universe is for programs written to the Java Virtual Machine. The vm universe facilitates the execution of a virtual machine. The docker universe runs a docker container as an HTCondor job. COMMANDS FOR MATCHMAKING rank = ClassAd Float Expression A ClassAd Floating-Point expression that states how to rank machines which have already met the requirements expression. Essentially, rank expresses preference.  A higher numeric value equals better rank. HTCondor will give the job the machine with the highest rank.  For example, requirements = Memory > 60 rank = Memory asks HTCondor to find all available machines with more than 60 megabytes of memory and give to the job the machine with the most amount of memory. The HTCondor User's Manual contains complete information on the syntax and available attributes that can be used in the ClassAd expression. request_cpus = num-cpus A requested number of CPUs (cores). If not specified, the number requested will be 1. If specified, the expression && (RequestCpus <= Target.Cpus) is appended to the requirements expression for the job. For pools that enable dynamic condor_startd provisioning, specifies the minimum number of CPUs requested for this job, resulting in a dynamic slot being created with this many cores. request_disk = quantity The requested amount of disk space in KiB requested for this job. If not specified, it will be set to the job ClassAd attribute DiskUsage. The expression && (RequestDisk <= Target.Disk) is appended to the requirements expression for the job. For pools that enable dynamic condor_startd provisioning, a dynamic slot will be created with at least this much disk space. Characters may be appended to a numerical value to indicate units. K or KB indicates KiB,  numbers of bytes. M or MB indicates MiB,  numbers of bytes. G or GB indicates GiB,  numbers of bytes. T or TB indicates TiB,  numbers of bytes. request_memory = quantity The required amount of memory in MiB that this job needs to avoid excessive swapping. If not specified and the submit command vm_memory is specified, then the value specified for vm_memory defines request_memory. If neither request_memory nor vm_memory is specified, the value is set by the configuration variable JOB_DEFAULT_REQUESTMEMORY. The actual amount of memory used by a job is represented by the job ClassAd attribute MemoryUsage. For pools that enable dynamic condor_startd provisioning, a dynamic slot will be created with at least this much RAM. The expression && (RequestMemory <= Target.Memory) is appended to the requirements expression for the job. Characters may be appended to a numerical value to indicate units. K or KB indicates KiB,  numbers of bytes. M or MB indicates MiB,  numbers of bytes. G or GB indicates GiB,  numbers of bytes. T or TB indicates TiB,  numbers of bytes. request_<name> = quantity The required amount of the custom machine resource identified by <name> that this job needs. The custom machine resource is defined in the machine's configuration. Machines that have available GPUs will define <name> to be GPUs. requirements = ClassAd Boolean Expression The requirements command is a boolean ClassAd expression which uses C-like operators. In order for any job in this cluster to run on a given machine, this requirements expression must evaluate to true on the given machine. For scheduler and local universe jobs, the requirements expression is evaluated against the Scheduler ClassAd which represents the the condor_schedd daemon running on the submit machine, rather than a remote machine. Like all commands in the submit description file, if multiple requirements commands are present, all but the last one are ignored. By default,\", u'appends the following clauses to the requirements expression: Arch and OpSys are set equal to the Arch and OpSys of the submit machine.  In other words: unless you request otherwise, HTCondor will give your job machines with the same architecture and operating system version as the machine running', u\". Cpus  RequestCpus, if the job ClassAd attribute RequestCpus is defined. Disk  RequestDisk, if the job ClassAd attribute RequestDisk is defined. Otherwise, Disk  DiskUsage is appended to the requirements. The DiskUsage attribute is initialized to the size of the executable plus the size of any files specified in a transfer_input_files command. It exists to ensure there is enough disk space on the target machine for HTCondor to copy over both the executable and needed input files. The DiskUsage attribute represents the maximum amount of total disk space required by the job in kilobytes. HTCondor automatically updates the DiskUsage attribute approximately every 20 minutes while the job runs with the amount of space being used by the job on the execute machine. Memory  RequestMemory, if the job ClassAd attribute RequestMemory is defined. If Universe is set to Vanilla, FileSystemDomain is set equal to the submit machine's FileSystemDomain. View the requirements of a job which has already been submitted (along with everything else about the job ClassAd) with the command condor_q -l; see the command reference for condor_q on page\\xa0.  Also, see the HTCondor Users Manual for complete information on the syntax and available attributes that can be used in the ClassAd expression. FILE TRANSFER COMMANDS dont_encrypt_input_files =  file1,file2,file... A comma and/or space separated list of input files that are not to be network encrypted when transferred with the file transfer mechanism. Specification of files in this manner overrides configuration that would use encryption. Each input file must also be in the list given by transfer_input_files. When a path to an input file or directory is specified, this specifies the path to the file on the submit side. A single wild card character (*) may be used in each file name. dont_encrypt_output_files =  file1,file2,file... A comma and/or space separated list of output files that are not to be network encrypted when transferred back with the file transfer mechanism. Specification of files in this manner overrides configuration that would use encryption. The output file(s) must also either be in the list given by transfer_output_files or be discovered and to be transferred back with the file transfer mechanism. When a path to an output file or directory is specified, this specifies the path to the file on the execute side. A single wild card character (*) may be used in each file name. encrypt_execute_directory = True |  False Defaults to False. If set to True, HTCondor will encrypt the contents of the remote scratch directory of the machine where the job is executed. This encryption is transparent to the job itself, but ensures that files left behind on the local disk of the execute machine, perhaps due to a system crash, will remain private. In addition,\", u'will append to the job\\'s requirements expression && (TARGET.HasEncryptExecuteDirectory) to ensure the job is matched to a machine that is capable of encrypting the contents of the execute directory. This support is limited to Windows platforms that use the NTFS file system and Linux platforms with the ecryptfs-utils package installed. encrypt_input_files =  file1,file2,file... A comma and/or space separated list of input files that are to be network encrypted when transferred with the file transfer mechanism. Specification of files in this manner overrides configuration that would not use encryption. Each input file must also be in the list given by transfer_input_files. When a path to an input file or directory is specified, this specifies the path to the file on the submit side. A single wild card character (*) may be used in each file name. The method of encryption utilized will be as agreed upon in security negotiation; if that negotiation failed, then the file transfer mechanism must also fail for files to be network encrypted. encrypt_output_files =  file1,file2,file... A comma and/or space separated list of output files that are to be network encrypted when transferred back with the file transfer mechanism. Specification of files in this manner overrides configuration that would not use encryption. The output file(s) must also either be in the list given by transfer_output_files or be discovered and to be transferred back with the file transfer mechanism. When a path to an output file or directory is specified, this specifies the path to the file on the execute side. A single wild card character (*) may be used in each file name. The method of encryption utilized will be as agreed upon in security negotiation; if that negotiation failed, then the file transfer mechanism must also fail for files to be network encrypted. max_transfer_input_mb = ClassAd Integer Expression This integer expression specifies the maximum allowed total size in MiB of the input files that are transferred for a job.  This expression does not apply to grid universe, standard universe, or files transferred via file transfer plug-ins.  The expression may refer to attributes of the job.  The special value -1 indicates no limit. If not defined, the value set by configuration variable MAX_TRANSFER_INPUT_MB is used. If the observed size of all input files at submit time is larger than the limit, the job will be immediately placed on hold with a HoldReasonCode value of 32. If the job passes this initial test, but the size of the input files increases or the limit decreases so that the limit is violated, the job will be placed on hold at the time when the file transfer is attempted. max_transfer_output_mb = ClassAd Integer Expression This integer expression specifies the maximum allowed total size in MiB of the output files that are transferred for a job.  This expression does not apply to grid universe, standard universe, or files transferred via file transfer plug-ins.  The expression may refer to attributes of the job.  The special value -1 indicates no limit. If not set, the value set by configuration variable MAX_TRANSFER_OUTPUT_MB is used. If the total size of the job\\'s output files to be transferred is larger than the limit, the job will be placed on hold with a HoldReasonCode value of 33. The output will be transferred up to the point when the limit is hit, so some files may be fully transferred, some partially, and some not at all. output_destination = destination-URL When present, defines a URL that specifies both a plug-in and a destination for the transfer of the entire output sandbox or a subset of output files as specified by the submit command transfer_output_files. The plug-in does the transfer of files, and no files are sent back to the submit machine. The HTCondor Administrator\\'s manual has full details. should_transfer_files = YES |  NO |  IF_NEEDED The should_transfer_files setting is used to define if HTCondor should transfer files to and from the remote machine where the job runs. The file transfer mechanism is used to run jobs which are not in the standard universe (and can therefore use remote system calls for file access) on machines which do not have a shared file system with the submit machine. should_transfer_files equal to YES will cause HTCondor to always transfer files for the job. NO disables HTCondor\\'s file transfer mechanism. IF_NEEDED will not transfer files for the job if it is matched with a resource in the same FileSystemDomain as the submit machine (and therefore, on a machine with the same shared file system). If the job is matched with a remote resource in a different FileSystemDomain, HTCondor will transfer the necessary files. For more information about this and other settings related to transferring files, see the HTCondor User\\'s manual section on the file transfer mechanism. Note that should_transfer_files is not supported for jobs submitted to the grid universe. skip_filechecks = True |  False When True, file permission checks for the submitted job are disabled. When False, file permissions are checked; this is the behavior when this command is not present in the submit description file. File permissions are checked for read permissions on all input files, such as those defined by commands input and transfer_input_files, and for write permission to output files, such as a log file defined by log and output files defined with output or transfer_output_files. stream_error = True |  False If True, then stderr is streamed back to the machine from which the job was submitted. If False, stderr is stored locally and transferred back when the job completes. This command is ignored if the job ClassAd attribute TransferErr is False. The default value is False. This command must be used in conjunction with error, otherwise stderr will sent to /dev/null on Unix machines and ignored on Windows machines. stream_input = True |  False If True, then stdin is streamed from the machine on which the job was submitted. The default value is False. The command is only relevant for jobs submitted to the vanilla or java universes, and it is ignored by the grid universe. This command must be used in conjunction with input, otherwise stdin will be /dev/null on Unix machines and ignored on Windows machines. stream_output = True |  False If True, then stdout is streamed back to the machine from which the job was submitted. If False, stdout is stored locally and transferred back when the job completes. This command is ignored if the job ClassAd attribute TransferOut is False. The default value is False. This command must be used in conjunction with output, otherwise stdout will sent to /dev/null on Unix machines and ignored on Windows machines. transfer_executable = True |  False This command is applicable to jobs submitted to the grid and vanilla universes. If transfer_executable is set to False, then HTCondor looks for the executable on the remote machine, and does not transfer the executable over. This is useful for an already pre-staged executable; HTCondor behaves more like rsh. The default value is True. transfer_input_files =  file1,file2,file... A comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started. By default, the file specified in the executable command and any file specified in the input command (for example, stdin) are transferred. When a path to an input file or directory is specified, this specifies the path to the file on the submit side. The file is placed in the job\\'s temporary scratch directory on the execute side, and it is named using the base name of the original path.  For example, /path/to/input_file becomes input_file in the job\\'s scratch directory. A directory may be specified by appending the forward slash character (/) as a trailing path separator. This syntax is used for both Windows and Linux submit hosts. A directory example using a trailing path separator is input_data/. When a directory is specified with the trailing path separator, the contents of the directory are transferred,  but the directory itself is not transferred. It is as if each of the items within the directory were listed in the transfer list. When there is no trailing path separator, the directory is transferred, its contents are transferred, and these contents are placed inside the transferred directory. For grid universe jobs other than HTCondor-C, the transfer of directories is not currently supported. Symbolic links to files are transferred as the files they point to. Transfer of symbolic links to directories is not currently supported. For vanilla and vm universe jobs only, a file may be specified by giving a URL, instead of a file name. The implementation for URL transfers requires both configuration and available plug-in. transfer_output_files =  file1,file2,file... This command forms an explicit list of output files and directories to be transferred back from the temporary working directory on the execute machine to the submit machine. If there are multiple files, they must be delimited with commas. Setting transfer_output_files to the empty string (\"\") means that no files are to be transferred. For HTCondor-C jobs and all other non-grid universe jobs, if transfer_output_files is not specified, HTCondor will automatically transfer back all files in the job\\'s temporary working directory which have been modified or created by the job.  Subdirectories are not scanned for output, so if output from subdirectories is desired, the output list must be explicitly specified. For grid universe jobs other than HTCondor-C, desired output files must also be explicitly listed. Another reason to explicitly list output files is for a job that creates many files, and the user wants only a subset transferred back. For grid universe jobs other than with grid type condor, to have files other than standard output and standard error transferred from the execute machine back to the submit machine, do use transfer_output_files, listing all files to be transferred. These files are found on the execute machine in the working directory of the job. When a path to an output file or directory is specified, it specifies the path to the file on the execute side. As a destination on the submit side, the file is placed in the job\\'s initial working directory, and it is named using the base name of the original path. For example, path/to/output_file becomes output_file in the job\\'s initial working directory. The name and path of the file that is written on the submit side may be modified by using transfer_output_remaps. Note that this remap function only works with files but not with directories. A directory may be specified using a trailing path separator. An example of a trailing path separator is the slash character on Unix platforms; a directory example using a trailing path separator is input_data/. When a directory is specified with a trailing path separator, the contents of the directory are transferred,  but the directory itself is not transferred. It is as if each of the items within the directory were listed in the transfer list. When there is no trailing path separator, the directory is transferred, its contents are transferred, and these contents are placed inside the transferred directory. For grid universe jobs other than HTCondor-C, the transfer of directories is not currently supported. Symbolic links to files are transferred as the files they point to. Transfer of symbolic links to directories is not currently supported. transfer_output_remaps   `` name  newname ; name2  newname2 ... \\'\\' This specifies the name (and optionally path) to use when downloading output files from the completed job.  Normally, output files are transferred back to the initial working directory with the same name they had in the execution directory.  This gives you the option to save them with a different path or name.  If you specify a relative path, the final path will be relative to the job\\'s initial working directory. name describes an output file name produced by your job, and newname describes the file name it should be downloaded to. Multiple remaps can be specified by separating each with a semicolon. If you wish to remap file names that contain equals signs or semicolons, these special characters may be escaped with a backslash. You cannot specify directories to be remapped. when_to_transfer_output =  ON_EXIT |  ON_EXIT_OR_EVICT Setting when_to_transfer_output equal to ON_EXIT will cause HTCondor to transfer the job\\'s output files back to the submitting machine only when the job completes (exits on its own). The ON_EXIT_OR_EVICT option is intended for fault tolerant jobs which periodically save their own state and can restart where they left off. In this case, files are spooled to the submit machine any time the job leaves a remote site, either because it exited on its own, or was evicted by the HTCondor system for any reason prior to job completion. The files spooled back are placed in a directory defined by the value of the SPOOL configuration variable. Any output files transferred back to the submit machine are automatically sent back out again as input files if the job restarts. POLICY COMMANDS hold = True |  False If hold is set to True, then the submitted job will be placed into the Hold state. Jobs in the Hold state will not run until released by condor_release. Defaults to False. keep_claim_idle = integer An integer number of seconds that a job requests the condor_schedd to wait before releasing its claim after the job exits or after the job is removed. The process by which the condor_schedd claims a condor_startd is somewhat time-consuming. To amortize this cost, the condor_schedd tries to reuse claims to run subsequent jobs, after a job using a claim is done. However, it can only do this if there is an idle job in the queue at the moment the previous job completes. Sometimes, and especially for the node jobs when using DAGMan, there is a subsequent job about to be submitted, but it has not yet arrived in the queue when the previous job completes. As a result, the condor_schedd releases the claim, and the next job must wait an entire negotiation cycle to start. When this submit command is defined with a non-negative integer, when the job exits, the condor_schedd tries as usual to reuse the claim. If it cannot, instead of releasing the claim, the condor_schedd keeps the claim until either the number of seconds given as a parameter, or a new job which matches that claim arrives, whichever comes first. The condor_startd in question will remain in the Claimed/Idle state, and the original job will be \"charged\" (in terms of priority) for the time in this state. leave_in_queue = ClassAd Boolean Expression When the ClassAd Expression evaluates to True, the job is not removed from the queue upon completion. This allows the user of a remotely spooled job to retrieve output files in cases where HTCondor would have removed them as part of the cleanup associated with completion. The job will only exit the queue once it has been marked for removal (via condor_rm, for example) and the leave_in_queue expression has become False. leave_in_queue defaults to False. As an example, if the job is to be removed once the output is retrieved with condor_transfer_data, then use leave_in_queue = (JobStatus == 4) && ((StageOutFinish =?= UNDEFINED) ||\\\\ (StageOutFinish == 0)) next_job_start_delay = ClassAd Boolean Expression This expression specifies the number of seconds to delay after starting up this job before the next job is started.  The maximum allowed delay is specified by the HTCondor configuration variable MAX_NEXT_JOB_START_DELAY, which defaults to 10 minutes. This command does not apply to scheduler or local universe jobs. This command has been historically used to implement a form of job start throttling from the job submitter\\'s perspective. It was effective for the case of multiple job submission where the transfer of extremely large input data sets to the execute machine caused machine performance to suffer. This command is no longer useful, as throttling should be accomplished through configuration of the condor_schedd daemon. on_exit_hold = ClassAd Boolean Expression The ClassAd expression is checked when the job exits, and if True, places the job into the Hold state. If False (the default value when not defined), then nothing happens and the on_exit_remove expression is checked to determine if that needs to be applied. For example: Suppose a job is known to run for a minimum of an hour. If the job exits after less than an hour, the job should be placed on hold and an e-mail notification sent, instead of being allowed to leave the queue. on_exit_hold = (time() - JobStartDate) < (60 * $(MINUTE)) This expression places the job on hold if it exits for any reason before running for an hour. An e-mail will be sent to the user explaining that the job was placed on hold because this expression became True. periodic_* expressions take precedence over on_exit_* expressions, and *_hold expressions take precedence over a *_remove expressions. Only job ClassAd attributes will be defined for use by this ClassAd expression. This expression is available for the vanilla, java, parallel, grid, local and scheduler universes. It is additionally available, when submitted from a Unix machine, for the standard universe. on_exit_hold_reason = ClassAd String Expression When the job is placed on hold due to the on_exit_hold expression becoming True, this expression is evaluated to set the value of HoldReason in the job ClassAd. If this expression is UNDEFINED or produces an empty or invalid string, a default description is used. on_exit_hold_subcode = ClassAd Integer Expression When the job is placed on hold due to the on_exit_hold expression becoming True, this expression is evaluated to set the value of HoldReasonSubCode in the job ClassAd. The default subcode is 0.  The HoldReasonCode will be set to 3, which indicates that the job went on hold due to a job policy expression. on_exit_remove = ClassAd Boolean Expression The ClassAd expression is checked when the job exits, and if True (the default value when undefined), then it allows the job to leave the queue normally. If False, then the job is placed back into the Idle state. If the user job runs under the vanilla universe, then the job restarts from the beginning. If the user job runs under the standard universe, then it continues from where it left off, using the last checkpoint. For example, suppose a job occasionally segfaults, but chances are that the job will finish successfully if the job is run again with the same data. The on_exit_remove expression can cause the job to run again with the following command. Assume that the signal identifier for the segmentation fault is 11 on the platform where the job will be running. on_exit_remove = (ExitBySignal == False) || (ExitSignal != 11) This expression lets the job leave the queue if the job was not killed by a signal or if it was killed by a signal other than 11, representing segmentation fault in this example. So, if the exited due to signal 11, it will stay in the job queue. In any other case of the job exiting, the job will leave the queue as it normally would have done. As another example, if the job should only leave the queue if it exited on its own with status 0, this on_exit_remove expression works well: on_exit_remove = (ExitBySignal == False) && (ExitCode == 0) If the job was killed by a signal or exited with a non-zero exit status, HTCondor would leave the job in the queue to run again. periodic_* expressions take precedence over on_exit_* expressions, and *_hold expressions take precedence over a *_remove expressions. Only job ClassAd attributes will be defined for use by this ClassAd expression. This expression is available for the vanilla, java, parallel, grid, local and scheduler universes. It is additionally available, when submitted from a Unix machine, for the standard universe.  Note that the condor_schedd daemon, by default, only checks these periodic expressions once every 300 seconds.  The period of these evaluations can be adjusted by setting the PERIODIC_EXPR_INTERVAL configuration macro. periodic_hold = ClassAd Boolean Expression This expression is checked periodically at an interval of the number of seconds set by the configuration variable PERIODIC_EXPR_INTERVAL. If it becomes True, the job will be placed on hold. If unspecified, the default value is False. periodic_* expressions take precedence over on_exit_* expressions, and *_hold expressions take precedence over a *_remove expressions. Only job ClassAd attributes will be defined for use by this ClassAd expression. This expression is available for the vanilla, java, parallel, grid, local and scheduler universes. It is additionally available, when submitted from a Unix machine, for the standard universe.  Note that the condor_schedd daemon, by default, only checks these periodic expressions once every 300 seconds.  The period of these evaluations can be adjusted by setting the PERIODIC_EXPR_INTERVAL configuration macro. periodic_hold_reason = ClassAd String Expression When the job is placed on hold due to the periodic_hold expression becoming True, this expression is evaluated to set the value of HoldReason in the job ClassAd. If this expression is UNDEFINED or produces an empty or invalid string, a default description is used. periodic_hold_subcode = ClassAd Integer Expression When the job is placed on hold due to the periodic_hold expression becoming true, this expression is evaluated to set the value of HoldReasonSubCode in the job ClassAd. The default subcode is 0.  The HoldReasonCode will be set to 3, which indicates that the job went on hold due to a job policy expression. periodic_release = ClassAd Boolean Expression This expression is checked periodically at an interval of the number of seconds set by the configuration variable PERIODIC_EXPR_INTERVAL while the job is in the Hold state. If the expression becomes True, the job will be released. Only job ClassAd attributes will be defined for use by this ClassAd expression. This expression is available for the vanilla, java, parallel, grid, local and scheduler universes. It is additionally available, when submitted from a Unix machine, for the standard universe.  Note that the condor_schedd daemon, by default, only checks periodic expressions once every 300 seconds.  The period of these evaluations can be adjusted by setting the PERIODIC_EXPR_INTERVAL configuration macro. periodic_remove = ClassAd Boolean Expression This expression is checked periodically at an interval of the number of seconds set by the configuration variable PERIODIC_EXPR_INTERVAL. If it becomes True, the job is removed from the queue. If unspecified, the default value is False. See the Examples section of this manual page for an example of a periodic_remove expression. periodic_* expressions take precedence over on_exit_* expressions, and *_hold expressions take precedence over a *_remove expressions. So, the periodic_remove expression takes precedent over the on_exit_remove expression, if the two describe conflicting actions. Only job ClassAd attributes will be defined for use by this ClassAd expression. This expression is available for the vanilla, java, parallel, grid, local and scheduler universes. It is additionally available, when submitted from a Unix machine, for the standard universe.  Note that the condor_schedd daemon, by default, only checks periodic expressions once every 300 seconds.  The period of these evaluations can be adjusted by setting the PERIODIC_EXPR_INTERVAL configuration macro. COMMANDS SPECIFIC TO THE STANDARD UNIVERSE allow_startup_script = True |  False If True, a standard universe job will execute a script instead of submitting the job, and the consistency check to see if the executable has been linked using condor_compile is omitted. The executable command within the submit description file specifies the name of the script. The script is used to do preprocessing before the job is submitted. The shell script ends with an exec of the job executable, such that the process id of the executable is the same as that of the shell script. Here is an example script that gets a copy of a machine-specific executable before the exec. #! /bin/sh # get the host name of the machine $host=`uname -n` # grab a standard universe executable designed specifically # for this host scp elsewhere@cs.wisc.edu:${host} executable # The PID MUST stay the same, so exec the new standard universe process. exec executable ${1+\"$@\"} If this command is not present (defined), then the value defaults to false. append_files = file1, file2, ... If your job attempts to access a file mentioned in this list, HTCondor will force all writes to that file to be appended to the end. Furthermore,', u'will not truncate it. This list uses the same syntax as compress_files, shown above. This option may yield some surprising results.  If several jobs attempt to write to the same file, their output may be intermixed. If a job is evicted from one or more machines during the course of its lifetime, such an output file might contain several copies of the results. This option should be only be used when you wish a certain file to be treated as a running log instead of a precise result. This option only applies to standard-universe jobs. buffer_files   `` name  (size,block-size) ; name2  (size,block-size) ... \\'\\' buffer_size  bytes-in-buffer buffer_block_size  bytes-in-block HTCondor keeps a buffer of recently-used data for each file a job accesses. This buffer is used both to cache commonly-used data and to consolidate small reads and writes into larger operations that get better throughput. The default settings should produce reasonable results for most programs. These options only apply to standard-universe jobs. If needed, you may set the buffer controls individually for each file using the buffer_files option. For example, to set the buffer size to 1 MiB and the block size to 256 KiB for the file input.data, use this command: buffer_files = \"input.data=(1000000,256000)\" Alternatively, you may use these two options to set the default sizes for all files used by your job: buffer_size = 1000000 buffer_block_size = 256000 If you do not set these, HTCondor will use the values given by these two configuration file macros: DEFAULT_IO_BUFFER_SIZE = 1000000 DEFAULT_IO_BUFFER_BLOCK_SIZE = 256000 Finally, if no other settings are present, HTCondor will use a buffer of 512 KiB and a block size of 32 KiB. compress_files = file1, file2, ... If your job attempts to access any of the files mentioned in this list, HTCondor will automatically compress them (if writing) or decompress them (if reading). The compress format is the same as used by GNU gzip. The files given in this list may be simple file names or complete paths and may include  as a wild card.  For example, this list causes the file /tmp/data.gz, any file named event.gz, and any file ending in .gzip to be automatically compressed or decompressed as needed: compress_files = /tmp/data.gz, event.gz, *.gzip Due to the nature of the compression format, compressed files must only be accessed sequentially.  Random access reading is allowed but is very slow, while random access writing is simply not possible.  This restriction may be avoided by using both compress_files and fetch_files at the same time.  When this is done, a file is kept in the decompressed state at the execution machine, but is compressed for transfer to its original location. This option only applies to standard universe jobs. fetch_files = file1, file2, ... If your job attempts to access a file mentioned in this list, HTCondor will automatically copy the whole file to the executing machine, where it can be accessed quickly.  When your job closes the file, it will be copied back to its original location. This list uses the same syntax as compress_files, shown above. This option only applies to standard universe jobs. file_remaps   `` name  newname ; name2  newname2 ... \\'\\' Directs HTCondor to use a new file name in place of an old one.  name describes a file name that your job may attempt to open, and newname describes the file name it should be replaced with. newname may include an optional leading access specifier, local: or remote:.  If left unspecified, the default access specifier is remote:.  Multiple remaps can be specified by separating each with a semicolon. This option only applies to standard universe jobs. If you wish to remap file names that contain equals signs or semicolons, these special characters may be escaped with a backslash. Example One: Suppose that your job reads a file named dataset.1. To instruct HTCondor to force your job to read other.dataset instead, add this to the submit file: file_remaps = \"dataset.1=other.dataset\" Example Two: Suppose that your run many jobs which all read in the same large file, called very.big. If this file can be found in the same place on a local disk in every machine in the pool, (say /bigdisk/bigfile,) you can instruct HTCondor of this fact by remapping very.big to /bigdisk/bigfile and specifying that the file is to be read locally, which will be much faster than reading over the network. file_remaps = \"very.big = local:/bigdisk/bigfile\" Example Three: Several remaps can be applied at once by separating each with a semicolon. file_remaps = \"very.big = local:/bigdisk/bigfile ; dataset.1 = other.dataset\" local_files = file1, file2, ... If your job attempts to access a file mentioned in this list, HTCondor will cause it to be read or written at the execution machine. This is most useful for temporary files not used for input or output. This list uses the same syntax as compress_files, shown above. local_files = /tmp/* This option only applies to standard universe jobs. want_remote_io = True |  False This option controls how a file is opened and manipulated in a standard universe job. If this option is true, which is the default, then the condor_shadow makes all decisions about how each and every file should be opened by the executing job. This entails a network round trip (or more) from the job to the condor_shadow and back again for every single open() in addition to other needed information about the file. If set to false, then when the job queries the condor_shadow for the first time about how to open a file, the condor_shadow will inform the job to automatically perform all of its file manipulation on the local file system on the execute machine and any file remapping will be ignored. This means that there must be a shared file system (such as NFS or AFS) between the execute machine and the submit machine and that ALL paths that the job could open on the execute machine must be valid. The ability of the standard universe job to checkpoint, possibly to a checkpoint server, is not affected by this attribute. However, when the job resumes it will be expecting the same file system conditions that were present when the job checkpointed. COMMANDS FOR THE GRID batch_queue = queuename Used for pbs, lsf, and sge grid universe jobs. Specifies the name of the PBS/LSF/SGE job queue into which the job should be submitted. If not specified, the default queue is used. boinc_authenticator_file = pathname For grid type boinc jobs, specifies a path and file name of the authorization file that grants permission for HTCondor to use the BOINC service. There is no default value when not specified. cream_attributes = name=value;... ;name=value Provides a list of attribute/value pairs to be set in a CREAM job description of a grid universe job destined for the CREAM grid system. The pairs are separated by semicolons, and written in New ClassAd syntax. delegate_job_GSI_credentials_lifetime = seconds Specifies the maximum number of seconds for which delegated proxies should be valid. The default behavior when this command is not specified is determined by the configuration variable DELEGATE_JOB_GSI_CREDENTIALS_LIFETIME, which defaults to one day. A value of 0 indicates that the delegated proxy should be valid for as long as allowed by the credential used to create the proxy.  This setting currently only applies to proxies delegated for non-grid jobs and for HTCondor-C jobs.  It does not currently apply to globus grid jobs, which always behave as though this setting were 0.  This variable has no effect if the configuration variable DELEGATE_JOB_GSI_CREDENTIALS is False, because in that case the job proxy is copied rather than delegated. deltacloud_hardware_profile = Deltacloud profile name Used for deltacloud jobs. An optional identifier for the type of VM desired. If not provided, a service-defined default is used. deltacloud_hardware_profile_cpu = cpu details Used for deltacloud jobs. An optional description of the CPUs desired for the VM, overriding the selected hardware profile. deltacloud_hardware_profile_memory = memory details Used for deltacloud jobs. An optional description of the memory (RAM) desired for the VM, overriding the selected hardware profile. deltacloud_hardware_profile_storage = storage details Used for deltacloud jobs. An optional description of the storage (disk) desired for the VM, overriding the selected hardware profile. deltacloud_image_id = Deltacloud image ID Used for deltacloud jobs. Identifier of the VM image to run. deltacloud_keyname = Deltacloud key name Used for deltacloud jobs. Identifier of the SSH key pair that should be used to allow remote login to the running instance. The key pair needs to be created before submission. deltacloud_password_file = pathname Used for deltacloud jobs. Path and file name of a file containing the secret key to be used to authenticate with a Deltacloud service. deltacloud_realm_id = Deltacloud realm ID Used for deltacloud jobs. An optional identifier specifying which of multiple locations within a cloud service should be used to run the VM. If not provided, a service-selected default is used. deltacloud_user_data = data Used for deltacloud jobs. A string, representing a block of data that can be accessed by the virtual machine job inside the cloud service. deltacloud_username = Deltacloud username Used for deltacloud jobs. The user name to be used to authenticate with a Deltacloud service. ec2_access_key_id = pathname For grid type ec2 jobs, identifies the file containing the access key. ec2_ami_id = EC2 xMI ID For grid type ec2 jobs, identifies the machine image. Services compatible with the EC2 Query API may refer to these with abbreviations other than AMI, for example EMI is valid for Eucalyptus. ec2_availability_zone = zone name For grid type ec2 jobs, specifies the Availability Zone that the instance should be run in. This command is optional, unless ec2_ebs_volumes is set. As an example, one current zone is us-east-1b. ec2_block_device_mapping = block-device:kernel-device,block-device:kernel-device, ... For grid type ec2 jobs, specifies the block device to kernel device mapping. This command is optional. ec2_ebs_volumes = ebs name:device name,ebs name:device name,... For grid type ec2 jobs, optionally specifies a list of Elastic Block Store (EBS) volumes to be made available to the instance and the device names they should have in the instance. ec2_elastic_ip = elastic IP address For grid type ec2 jobs, and optional specification of an Elastic IP address that should be assigned to this instance. ec2_iam_profile_arn = IAM profile ARN For grid type ec2 jobs, an Amazon Resource Name (ARN) identifying which Identity and Access Management (IAM) (instance) profile to associate with the instance. ec2_iam_profile_name= IAM profile name For grid type ec2 jobs, a name identifying which Identity and Access Management (IAM) (instance) profile to associate with the instance. ec2_instance_type = instance type For grid type ec2 jobs, identifies the instance type. Different services may offer different instance types, so no default value is set. ec2_key_pair = ssh key-pair name For grid type ec2 jobs, specifies the name of an SSH key-pair that is already registered with the EC2 service. The associated private key can be used to ssh into the virtual machine once it is running. ec2_key_pair_file = pathname For grid type ec2 jobs, specifies the complete path and file name of a file into which HTCondor will write an SSH key for use with ec2 jobs. The key can be used to ssh into the virtual machine once it is running. If ec2_keypair is specified for a job, ec2_keypair_file is ignored. ec2_parameter_names = ParameterName1, ParameterName2, ... For grid type ec2 jobs, a space or comma separated list of the names of additional parameters to pass when instantiating an instance. ec2_parameter_name = value For grid type ec2 jobs, specifies the value for the correspondingly named (instance instantiation) parameter. <name> is the parameter name specified in the submit command ec2_parameter_names, but with any periods replaced by underscores. ec2_secret_access_key = pathname For grid type ec2 jobs, specifies the path and file name containing the secret access key. ec2_security_groups = group1, group2, ... For grid type ec2 jobs, defines the list of EC2 security groups which should be associated with the job. ec2_security_ids = id1, id2, ... For grid type ec2 jobs, defines the list of EC2 security group IDs which should be associated with the job. ec2_spot_price = bid For grid type ec2 jobs, specifies the spot instance bid, which is the most that the job submitter is willing to pay per hour to run this job. ec2_tag_names = name0,name1,name... For grid type ec2 jobs, specifies the case of tag names that will be associated with the running instance. This is only necessary if a tag name case matters. By default the list will be automatically generated. ec2_tag_name = value For grid type ec2 jobs, specifies a tag to be associated with the running instance. The tag name will be lower-cased, use ec2_tag_names to change the case. ec2_user_data = data For grid type ec2 jobs, provides a block of data that can be accessed by the virtual machine. If both ec2_user_data and ec2_user_data_file are specified for a job, the two blocks of data are concatenated, with the data from this ec2_user_data submit command occurring first. ec2_user_data_file = pathname For grid type ec2 jobs, specifies a path and file name whose contents can be accessed by the virtual machine. If both ec2_user_data and ec2_user_data_file are specified for a job, the two blocks of data are concatenated, with the data from that ec2_user_data submit command occurring first. ec2_vpc_ip = a.b.c.d For grid type ec2 jobs, that are part of a Virtual Private Cloud (VPC), an optional specification of the IP address that this instance should have within the VPC. ec2_vpc_subnet = subnet specification string For grid type ec2 jobs, an optional specification of the Virtual Private Cloud (VPC) that this instance should be a part of. gce_auth_file = pathname For grid type gce jobs, specifies a path and file name of the authorization file that grants permission for HTCondor to use the Google account. There is no default value when not specified. gce_image = URL For grid type gce jobs, the URL of the virtual machine image representing the HTCondor job to be run.  This virtual machine image must already be register with GCE and reside in Google\\'s Cloud Storage service. gce_machine_type = machine type For grid type gce jobs, the long form of the URL that describes the machine configuration that the virtual machine instance is to run on. gce_metadata = name=value,... ,name=value For grid type gce jobs, a comma separated list of name and value pairs that define metadata for a virtual machine instance that is an HTCondor job. gce_metadata_file = pathname For grid type gce jobs, specifies a path and file name of the file that contains metadata for a virtual machine instance that is an HTCondor job. Within the file, each name and value pair is on its own line; so, the pairs are separated by the newline character. globus_rematch = ClassAd Boolean Expression This expression is evaluated by the condor_gridmanager whenever: the globus_resubmit expression evaluates to True the condor_gridmanager decides it needs to retry a submission (as when a previous submission failed to commit) If globus_rematch evaluates to True, then before the job is submitted again to globus, the condor_gridmanager will request that the condor_schedd daemon renegotiate with the matchmaker (the condor_negotiator). The result is this job will be matched again. globus_resubmit = ClassAd Boolean Expression The expression is evaluated by the condor_gridmanager each time the condor_gridmanager gets a job ad to manage. Therefore, the expression is evaluated: when a grid universe job is first submitted to HTCondor-G when a grid universe job is released from the hold state when HTCondor-G is restarted (specifically, whenever the condor_gridmanager is restarted) If the expression evaluates to True, then any previous submission to the grid universe will be forgotten and this job will be submitted again as a fresh submission to the grid universe. This may be useful if there is a desire to give up on a previous submission and try again. Note that this may result in the same job running more than once.  Do not treat this operation lightly. globus_rsl = RSL-string Used to provide any additional Globus RSL string attributes which are not covered by other submit description file commands or job attributes. Used for grid universe jobs, where the grid resource has a grid-type-string of gt2. grid_resource = grid-type-string grid-specific-parameter-list For each grid-type-string value, there are further type-specific values that must specified. This submit description file command allows each to be given in a space-separated list. Allowable grid-type-string values are batch, condor, cream, deltacloud, ec2, gt2, gt5, lsf, nordugrid, pbs, sge, and unicore. The HTCondor manual chapter on Grid Computing details the variety of grid types. For a grid-type-string of batch, the single parameter is the name of the local batch system, and will be one of pbs, lsf, or sge. For a grid-type-string of condor, the first parameter is the name of the remote condor_schedd daemon. The second parameter is the name of the pool to which the remote condor_schedd daemon belongs. For a grid-type-string of cream, there are three parameters. The first parameter is the web services address of the CREAM server. The second parameter is the name of the batch system that sits behind the CREAM server. The third parameter identifies a site-specific queue within the batch system. For a grid-type-string of deltacloud, the single parameter is the URL of the deltacloud service requested. For a grid-type-string of ec2, one additional parameter specifies the EC2 URL. For a grid-type-string of gt2, the single parameter is the name of the pre-WS GRAM resource to be used. For a grid-type-string of gt5, the single parameter is the name of the pre-WS GRAM resource to be used, which is the same as for the grid-type-string of gt2. For a grid-type-string of lsf, no additional parameters are used. For a grid-type-string of nordugrid, the single parameter is the name of the NorduGrid resource to be used. For a grid-type-string of pbs, no additional parameters are used. For a grid-type-string of sge, no additional parameters are used. For a grid-type-string of unicore, the first parameter is the name of the Unicore Usite to be used. The second parameter is the name of the Unicore Vsite to be used. keystore_alias = name A string to locate the certificate in a Java keystore file, as used for a unicore job. keystore_file = pathname The complete path and file name of the Java keystore file containing the certificate to be used for a unicore job. keystore_passphrase_file = pathname The complete path and file name to the file containing the passphrase protecting a Java keystore file containing the certificate. Relevant for a unicore job. MyProxyCredentialName = symbolic name The symbolic name that identifies a credential to the MyProxy server. This symbolic name is set as the credential is initially stored on the server (using myproxy-init). MyProxyHost = host:port The Internet address of the host that is the MyProxy server. The host may be specified by either a host name (as in head.example.com) or an IP address (of the form 123.456.7.8). The port number is an integer. MyProxyNewProxyLifetime = number-of-minutes The new lifetime (in minutes) of the proxy after it is refreshed. MyProxyPassword = password The password needed to refresh a credential on the MyProxy server. This password is set when the user initially stores credentials on the server (using myproxy-init). As an alternative to using MyProxyPassword in the submit description file, the password may be specified as a command line argument to', u\"with the -password argument. MyProxyRefreshThreshold = number-of-seconds The time (in seconds) before the expiration of a proxy that the proxy should be refreshed. For example, if MyProxyRefreshThreshold is set to the value 600, the proxy will be refreshed 10 minutes before it expires. MyProxyServerDN = credential subject A string that specifies the expected Distinguished Name (credential subject, abbreviated DN) of the MyProxy server. It must be specified when the MyProxy server DN does not follow the conventional naming scheme of a host credential. This occurs, for example, when the  MyProxy server DN begins with a user credential. nordugrid_rsl = RSL-string Used to provide any additional RSL string attributes which are not covered by regular submit description file parameters. Used when the universe is grid, and the type of grid system is nordugrid. transfer_error = True |  False For jobs submitted to the grid universe only. If True, then the error output (from stderr) from the job is transferred from the remote machine back to the submit machine. The name of the file after transfer is given by the error command. If False, no transfer takes place (from the remote machine to submit machine), and the name of the file is given by the error command. The default value is True. transfer_input = True |  False For jobs submitted to the grid universe only. If True, then the job input (stdin) is transferred from the machine where the job was submitted to the remote machine. The name of the file that is transferred is given by the input command. If False, then the job's input is taken from a pre-staged file on the remote machine, and the name of the file is given by the input command. The default value is True. For transferring files other than stdin, see transfer_input_files. transfer_output = True |  False For jobs submitted to the grid universe only. If True, then the output (from stdout) from the job is transferred from the remote machine back to the submit machine. The name of the file after transfer is given by the output command. If False, no transfer takes place (from the remote machine to submit machine), and the name of the file is given by the output command. The default value is True. For transferring files other than stdout, see transfer_output_files. use_x509userproxy = True |  False Set this command to True to indicate that the job requires an X.509 user proxy. If x509userproxy is set, then that file is used for the proxy. Otherwise, the proxy is looked for in the standard locations. If x509userproxy is set or if the job is a grid universe job of grid type gt2, gt5, cream, or nordugrid, then the value of use_x509userproxy is forced to True. Defaults to False. x509userproxy = full-pathname Used to override the default path name for X.509 user certificates. The default location for X.509 proxies is the /tmp directory, which is generally a local file system. Setting this value would allow HTCondor to access the proxy in a shared file system (for example, AFS). HTCondor will use the proxy specified in the submit description file first. If nothing is specified in the submit description file, it will use the environment variable X509_USER_PROXY. If that variable is not present, it will search in the default location. x509userproxy is relevant when the universe is vanilla, or when the universe is grid and the type of grid system is one of gt2, gt5, condor, cream, or nordugrid. Defining a value causes the proxy to be delegated to the execute machine. Further, VOMS attributes defined in the proxy will appear in the job ClassAd. COMMANDS FOR PARALLEL, JAVA, and SCHEDULER UNIVERSES hold_kill_sig = signal-number For the scheduler universe only, signal-number is the signal delivered to the job when the job is put on hold with condor_hold. signal-number may be either the platform-specific name or value of the signal. If this command is not present, the value of kill_sig is used. jar_files = file_list Specifies a list of additional JAR files to include when using the Java universe.  JAR files will be transferred along with the executable and automatically added to the classpath. java_vm_args = argument_list Specifies a list of additional arguments to the Java VM itself, When HTCondor runs the Java program, these are the arguments that go before the class name.  This can be used to set VM-specific arguments like stack size, garbage-collector arguments and initial property values. machine_count = max For the parallel universe, a single value (max) is required. It is neither a maximum or minimum, but the number of machines to be dedicated toward running the job. remove_kill_sig = signal-number For the scheduler universe only, signal-number is the signal delivered to the job when the job is removed with condor_rm. signal-number may be either the platform-specific name or value of the signal. This example shows it both ways for a Linux signal: remove_kill_sig = SIGUSR1 remove_kill_sig = 10 If this command is not present, the value of kill_sig is used. COMMANDS FOR THE VM UNIVERSE vm_disk = file1:device1:permission1, file2:device2:permission2:format2, ... A list of comma separated disk files. Each disk file is specified by 4 colon separated fields. The first field is the path and file name of the disk file. The second field specifies the device. The third field specifies permissions, and the optional fourth field specifies the image format. An example that specifies two disk files: vm_disk = /myxen/diskfile.img:sda1:w,/myxen/swap.img:sda2:w vm_checkpoint = True |  False A boolean value specifying whether or not to take checkpoints. If not specified, the default value is False. In the current implementation, setting both vm_checkpoint and vm_networking to True does not yet work in all cases. Networking cannot be used if a vm universe job uses a checkpoint in order to continue execution after migration to another machine. vm_macaddr = MACAddr Defines that MAC address that the virtual machine's network interface should have, in the standard format of six groups of two hexadecimal digits separated by colons. vm_memory = MBytes-of-memory The amount of memory in MBytes that a vm universe job requires. vm_networking = True |  False Specifies whether to use networking or not. In the current implementation, setting both vm_checkpoint and vm_networking to True does not yet work in all cases. Networking cannot be used if a vm universe job uses a checkpoint in order to continue execution after migration to another machine. vm_networking_type = nat |  bridge When vm_networking is True, this definition augments the job's requirements to match only machines with the specified networking. If not specified, then either networking type matches. vm_no_output_vm = True |  False When True, prevents HTCondor from transferring output files back to the machine from which the vm universe job was submitted. If not specified, the default value is False. vm_type = vmware |  xen |  kvm Specifies the underlying virtual machine software that this job expects. vmware_dir = pathname The complete path and name of the directory where VMware-specific files and applications such as the VMDK (Virtual Machine Disk Format) and VMX (Virtual Machine Configuration) reside. This command is optional; when not specified, all relevant VMware image files are to be listed using transfer_input_files. vmware_should_transfer_files = True |  False Specifies whether HTCondor will transfer VMware-specific files located as specified by vmware_dir to the execute machine (True) or rely on access through a shared file system (False). Omission of this required command (for VMware vm universe jobs) results in an error message from\", u\", and the job will not be submitted. vmware_snapshot_disk = True |  False When True, causes HTCondor to utilize a VMware snapshot disk for new or modified files. If not specified, the default value is True. xen_initrd = image-file When xen_kernel gives a path and file name for the kernel image to use, this optional command may specify a path to and ramdisk (initrd) image file. xen_kernel = included |   path-to-kernel A value of included specifies that the kernel is included in the disk file. If not one of these values, then the value is a path and file name of the kernel to be used. xen_kernel_params = string A string that is appended to the Xen kernel command line. xen_root = string A string that is appended to the Xen kernel command line to specify the root device. This string is required when xen_kernel gives a path to a kernel.  Omission for this required case results in an error message during submission. COMMANDS FOR THE DOCKER UNIVERSE docker_image =  image-name Defines the name of the Docker image that is the basis for the docker container. ADVANCED COMMANDS accounting_group = accounting-group-name Causes jobs to negotiate under the given accounting group. This value is advertised in the job ClassAd as AcctGroup. The HTCondor Administrator's manual  contains more information about accounting groups. accounting_group_user = accounting-group-user-name Sets the user name associated with the accounting group name for resource usage accounting purposes.  If not set, defaults to the value of the job ClassAd attribute Owner. This value is advertised in the job ClassAd as AcctGroupUser. If an accounting group has not been set with the accounting_group command, this command is ignored. concurrency_limits = string-list A list of resources that this job needs. The resources are presumed to have concurrency limits placed upon them, thereby limiting the number of concurrent jobs in execution which need the named resource. Commas and space characters delimit the items in the list. Each item in the list is a string that identifies the limit, or it is a ClassAd expression that evaluates to a string, and it is evaluated in the context of machine ClassAd being considered as a match. Each item in the list also may specify a numerical value identifying the integer number of resources required for the job. The syntax follows the resource name by a colon character (:) and the numerical value. Details on concurrency limits are in the HTCondor Administrator's manual. concurrency_limits_expr = ClassAd String Expression A ClassAd expression that represents the list of resources that this job needs after evaluation. The ClassAd expression may specify machine ClassAd attributes that are evaluated against a matched machine. After evaluation, the list sets concurrency_limits. copy_to_spool = True |  False If copy_to_spool is True, then\", u'copies the executable to the local spool directory before running it on a remote host. As copying can be quite time consuming and unnecessary, the default value is False for all job universes other than the standard universe. When False,', u\"does not copy the executable to a local spool directory. The default is True in standard universe, because resuming execution from a checkpoint can only be guaranteed to work using precisely the same executable that created the checkpoint. coresize = size Should the user's program abort and produce a core file, coresize specifies the maximum size in bytes of the core file which the user wishes to keep. If coresize is not specified in the command file, the system's user resource limit coredumpsize is used. A value of -1 results in no limits being applied to the core file size. cron_day_of_month = Cron-evaluated Day The set of days of the month for which a deferral time applies. The HTCondor User's manual section on Time Scheduling for Job Execution has further details. cron_day_of_week = Cron-evaluated Day The set of days of the week for which a deferral time applies. The HTCondor User's manual section on Time Scheduling for Job Execution has further details. cron_hour = Cron-evaluated Hour The set of hours of the day for which a deferral time applies. The HTCondor User's manual section on Time Scheduling for Job Execution has further details. cron_minute = Cron-evaluated Minute The set of minutes within an hour for which a deferral time applies. The HTCondor User's manual section on Time Scheduling for Job Execution has further details. cron_month = Cron-evaluated Month The set of months within a year for which a deferral time applies. The HTCondor User's manual section on Time Scheduling for Job Execution has further details. cron_prep_time = ClassAd Integer Expression Analogous to deferral_prep_time. The number of seconds prior to a job's deferral time that the job may be matched and sent to an execution machine. cron_window = ClassAd Integer Expression Analogous to the submit command deferral_window. It allows cron jobs that miss their deferral time to begin execution. The HTCondor User's manual section on Time Scheduling for Job Execution has further details. dagman_log = pathname DAGMan inserts this command to specify an event log that it watches to maintain the state of the DAG. If the log command is not specified in the submit file, DAGMan uses the log command to specify the event log. deferral_prep_time = ClassAd Integer Expression The number of seconds prior to a job's deferral time that the job may be matched and sent to an execution machine. The HTCondor User's manual section on Time Scheduling for Job Execution has further details. deferral_time = ClassAd Integer Expression Allows a job to specify the time at which its execution is to begin, instead of beginning execution as soon as it arrives at the execution machine. The deferral time is an expression that evaluates to a Unix Epoch timestamp (the number of seconds elapsed since 00:00:00 on January 1, 1970, Coordinated Universal Time). Deferral time is evaluated with respect to the execution machine. This option delays the start of execution, but not the matching and claiming of a machine for the job. If the job is not available and ready to begin execution at the deferral time, it has missed its deferral time. A job that misses its deferral time will be put on hold in the queue. The HTCondor User's manual section on Time Scheduling for Job Execution has further details. Due to implementation details, a deferral time may not be used for scheduler universe jobs. deferral_window = ClassAd Integer Expression The deferral window is used in conjunction with the deferral_time command to allow jobs that miss their deferral time to begin execution. The HTCondor User's manual section on Time Scheduling for Job Execution has further details.\"]\n",
      "condor_submit_dag Manage and queue jobs within a specified DAG for execution on remote machines [u'[-help | -version]', u'[-no_submit] [-verbose] [-force] [-maxidle  NumberOfJobs] [-maxjobs  NumberOfJobs] [-dagman  DagmanExecutable] [-maxpre  NumberOfPREscripts] [-maxpost  NumberOfPOSTscripts] [-notification  value] [-noeventchecks] [-allowlogerror] [-r  schedd_name] [-debug  level] [-usedagdir] [-outfile_dir  directory] [-config  ConfigFileName] [-insert_sub_file  FileName] [-append  Command] [-autorescue  0|1] [-dorescuefrom  number] [-allowversionmismatch] [-no_recurse] [-do_recurse] [-update_submit] [-import_env] [-DumpRescue] [-valgrind] [-DontAlwaysRunPost] [-priority  number] [-dont_use_default_node_log] [-schedd-daemon-ad-file  FileName] [-schedd-address-file  FileName] [-suppress_notification] [-dont_suppress_notification] [-DoRecovery] DAGInputFile1 [DAGInputFile2 ... DAGInputFileN ] Description', u\"is the program for submitting a DAG (directed acyclic graph) of jobs for execution under HTCondor. The program enforces the job dependencies defined in one or more DAGInputFiles. Each DAGInputFile contains commands to direct the submission of jobs implied by the nodes of a DAG to HTCondor. Extensive documentation is in the HTCondor User Manual section on DAGMan. Some options may be specified on the command line or in the configuration or in a node job's submit description file. Precedence is given to command line options or configuration over settings from a submit description file. An example is e-mail notifications. When configuration variable DAGMAN_SUPPRESS_NOTIFICATION is its default value of True, and a node job's submit description file contains notification = Complete e-mail will not be sent upon completion, as the value of DAGMAN_SUPPRESS_NOTIFICATION is enforced. Options -help Display usage information and exit. -version Display version information and exit. -no_submit Produce the HTCondor submit description file for DAGMan, but do not submit DAGMan as an HTCondor job. -verbose Cause\", u'to give verbose error messages. -force Require', u\"to overwrite the files that it produces, if the files already exist.  Note that dagman.out will be appended to, not overwritten.  If new-style rescue DAG mode is in effect, and any new-style rescue DAGs exist, the -force flag will cause them to be renamed, and the original DAG will be run. If old-style rescue DAG mode is in effect, any existing old-style rescue DAGs will be deleted, and the original DAG will be run. -maxidle NumberOfJobs Sets the maximum number of idle jobs allowed before condor_dagman stops submitting more jobs.  Once idle jobs start to run, condor_dagman will resume submitting jobs. NumberOfJobs is a positive integer. If the option is omitted, the number of idle jobs is defined by configuration variable DAGMAN_MAX_JOBS_IDLE, which defaults to 1000. Note that for this argument, each individual process within a cluster counts as a job, which is inconsistent with -maxjobs . Nothing special is done to the submit description file. Setting queue 5000 in the submit description file, where -maxidle is set to 250 will result in a cluster of 5000 new jobs being submitted to the condor_schedd. In this case, condor_dagman will resume submitting jobs when the number of idle jobs falls below 250. -maxjobs NumberOfJobs Sets the maximum number of jobs within the DAG that will be submitted to HTCondor at one time. NumberOfJobs is a positive integer. If the option is omitted, the default number of jobs is unlimited.  Note that for this argument, each cluster counts as one job, no matter how many individual processes are in the cluster. -dagman DagmanExecutable Allows the specification of an alternate condor_dagman executable to be used instead of the one found in the user's path. This must be a fully qualified path. -maxpre NumberOfPREscripts Sets the maximum number of PRE scripts within the DAG that may be running at one time. NumberOfPREScripts is a positive integer. If this option is omitted, the default number of PRE scripts is unlimited. -maxpost NumberOfPOSTscripts Sets the maximum number of POST scripts within the DAG that may be running at one time. NumberOfPOSTScripts is a positive integer. If this option is omitted, the default number of POST scripts is unlimited. -notification value Sets the e-mail notification for DAGMan itself. This information will be used within the HTCondor submit description file for DAGMan. This file is produced by\", u\". See the description of notification within condor_submit manual page for a specification of value. -noeventchecks This argument is no longer used; it is now ignored.  Its functionality is now implemented by the DAGMAN_ALLOW_EVENTS configuration variable. -allowlogerror This optional argument has condor_dagman try to run the specified DAG, even in the case of detected errors in the job event log specification. As of version 7.3.2, this argument has an effect only on DAGs containing Stork job nodes. -r schedd_name Submit condor_dagman to a remote machine, specifically the condor_schedd daemon on that machine. The condor_dagman job will not run on the local condor_schedd (the submit machine), but on the specified one. This is implemented using the -remote option to condor_submit. Note that this option does not currently specify input files for condor_dagman, nor the individual nodes to be taken along! It is assumed that any necessary files will be present on the remote computer, possibly via a shared file system between the local computer and the remote computer. It is also necessary that the user has appropriate permissions to submit a job to the remote machine; the permissions are the same as those required to use condor_submit's -remote option. If other options are desired, including transfer of other input files, consider using the -no_submit option, modifying the resulting submit file for specific needs, and then using condor_submit on that. -debug level Passes the the level of debugging output desired to condor_dagman.  level is an integer, with values of 0-7 inclusive, where 7 is the most verbose output. See the condor_dagman manual page for detailed descriptions of these values. If not specified, no -debug value is passed to condor_dagman. -usedagdir This optional argument causes condor_dagman to run each specified DAG as if\", u'had been run in the directory containing that DAG file.  This option is most useful when running multiple DAGs in a single condor_dagman. Note that the -usedagdir flag must not be used when running an old-style Rescue DAG. -outfile_dir directory Specifies the directory in which the .dagman.out file will be written.  The directory may be specified relative to the current working directory as', u'is executed, or specified with an absolute path. Without this option, the .dagman.out file is placed in the same directory as the first DAG input file listed on the command line. -config ConfigFileName Specifies a configuration file to be used for this DAGMan run. Note that the options specified in the configuration file apply to all DAGs if multiple DAGs are specified.  Further note that it is a fatal error if the configuration file specified by this option conflicts with a configuration file specified in any of the DAG files, if they specify one. -insert_sub_file FileName Specifies a file to insert into the .condor.sub file created by', u'.  The specified file must contain only legal submit file commands.  Only one file can be inserted.  (If both the DAGMAN_INSERT_SUB_FILE configuration variable and -insert_sub_file are specified, -insert_sub_file overrides DAGMAN_INSERT_SUB_FILE.)  The specified file is inserted into the .condor.sub file before the Queue command and before any commands specified with the -append option. -append Command Specifies a command to append to the .condor.sub file created by', u'.  The specified command is appended to the .condor.sub file immediately before the Queue command. Multiple commands are specified by using the -append option multiple times. Each new command is given in a separate -append option. Commands with spaces in them must be enclosed in double quotes. Commands specified with the -append option are appended to the .condor.sub file after commands inserted from a file specified by the -insert_sub_file option or the DAGMAN_INSERT_SUB_FILE configuration variable, so the -append command(s) will override commands from the inserted file if the commands conflict. -autorescue 0|1 Whether to automatically run the newest rescue DAG for the given DAG file, if one exists (0 = false, 1 = true). -dorescuefrom number Forces condor_dagman to run the specified rescue DAG number for the given DAG.  A value of 0 is the same as not specifying this option.  Specifying a non-existent rescue DAG is a fatal error. -allowversionmismatch This optional argument causes condor_dagman to allow a version mismatch between condor_dagman itself and the .condor.sub file produced by', u'(or, in other words, between', u'and condor_dagman).  WARNING!  This option should be used only if absolutely necessary.  Allowing version mismatches can cause subtle problems when running DAGs. (Note that, starting with version 7.4.0, condor_dagman no longer requires an exact version match between itself and the .condor.sub file.  Instead, a \"minimum compatible version\" is defined, and any .condor.sub file of that version or newer is accepted.) -no_recurse This optional argument causes', u'to not run itself recursively on nested DAGs (this is now the default; this flag has been kept mainly for backwards compatibility). -do_recurse This optional argument causes', u'to run itself recursively on nested DAGs. The default is now that it does not run itself recursively; instead the .condor.sub files for nested DAGs are generated \"lazily\" by condor_dagman itself.  DAG nodes specified with the SUBDAG EXTERNAL keyword or with submit file names ending in .condor.sub are considered nested DAGs. The DAGMAN_GENERATE_SUBDAG_SUBMITS configuration variable may be relevant. -update_submit This optional argument causes an existing .condor.sub file to not be treated as an error; rather, the .condor.sub file will be overwritten, but the existing values of -maxjobs, -maxidle, -maxpre, and -maxpost will be preserved. -import_env This optional argument causes', u'to import the current environment into the environment command of the .condor.sub file it generates. -DumpRescue This optional argument tells condor_dagman to immediately dump a rescue DAG and then exit, as opposed to actually running the DAG.  This feature is mainly intended for testing.  The Rescue DAG file is produced whether or not there are parse errors reading the original DAG input file. The name of the file differs if there was a parse error. -valgrind This optional argument causes the submit description file generated for the submission of condor_dagman to be modified. The executable becomes valgrind run on condor_dagman, with a specific set of arguments intended for testing condor_dagman. Note that this argument is intended for testing purposes only. Using the -valgrind option without the necessary valgrind software installed will cause the DAG to fail. If the DAG does run, it will run much more slowly than usual. -DontAlwaysRunPost This option causes the submit']\n",
      "condor_suspend suspend jobs from the HTCondor queue [u'[-help  -version]', u'[-debug] [-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] cluster  cluster.process  user -constraint expression  -all']\n",
      "condor_tail Display the last contents of a running job's standard output or file [u'[-help] | [-version]', u'[-pool  centralmanagerhostname[:portnumber]] [-name  name] [-debug] [-maxbytes  numbytes] [-auto-retry] [-follow] [-no-stdout] [-stderr] job-ID [-filename1] [, -filename2 ... ]']\n",
      "condor_transfer_data transfer spooled data [u'[-help  -version]', u'[-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] cluster...  cluster.process...  user... -constraint expression ...', u'[-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] -all']\n",
      "condor_update_machine_ad update a machine ClassAd [u'[-help  -version]', u'[-pool  centralmanagerhostname[:portnumber]] [-name  startdname] path/to/update-ad']\n",
      "condor_updates_stats Display output from condor_status [u'[--help  -h]  [--version]', u'[--long  -l] [--history=min-max] [--interval=seconds] [--notime] [--time] [--summary  -s]']\n",
      "condor_urlfetch fetch configuration given a URL [u'[-<daemon>] url local-url-cache-file']\n",
      "condor_userlog Display and summarize job statistics from job log files. [u'[-help] [-total | -raw] [-debug] [-evict] [-j  cluster | cluster.proc] [-all] [-hostname] logfile ...']\n",
      "condor_userprio Manage user priorities [u'-help', u'[-pool  centralmanagerhostname[:portnumber]] [Edit option]  [Display options] [-inputfile  filename]']\n",
      "condor_vacate Vacate jobs that are running on the specified hosts [u'[-help  -version]', u'[-graceful  -fast] [-debug] [-pool  centralmanagerhostname[:portnumber]] [-name hostname  hostname -addr \"a.b.c.d:port\"  \"a.b.c.d:port\" -constraint expression  -all ]']\n",
      "condor_vacate_job vacate jobs in the HTCondor queue from the hosts where they are running [u'[-help  -version]', u'[-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] [-fast] cluster...  cluster.process...  user... -constraint expression ...', u'[-pool centralmanagerhostname[:portnumber] -name scheddname ] [-addr  \"a.b.c.d:port\"] [-fast] -all']\n",
      "condor_version print HTCondor version and platform information [u'[-help]', u'[-arch] [-opsys] [-syscall]']\n",
      "condor_wait Wait for jobs to finish [u'[-help  -version]', u'[-debug] [-status] [-echo] [-wait  seconds] [-num  number-of-jobs] log-file [job ID]']\n",
      "condor_who Display information about owners of jobs and jobs running on an execute machine [u'[help options] [address options] [display options]']\n",
      "gidd_alloc find a GID within the specified range which is not used by any process [u'min-gid max-gid']\n",
      "procd_ctl command line interface to the condor_procd [u'-h', u'-A address-file [command]']\n"
     ]
    }
   ],
   "source": [
    "for c in commands:\n",
    "    print c.name, c.brief, c.synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('dump.json', 'w') as json_file:\n",
    "    json_file.write(json.JSONEncoder(indent=1).encode([c.__dict__ for c in commands]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
